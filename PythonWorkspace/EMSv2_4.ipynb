{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julian-Banks/EEE4022S_BNKJUL001_Thesis/blob/main/PythonWorkspace/EMSv2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V6yT1Kii6fZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Version Notes**\n",
        "\n",
        "# **v2.4**\n",
        "* going to have shorter episodes (one month long)\n",
        "* hope the agent can learn to expect the demand charge every 30 days.\n",
        "\n",
        "# **v2.3**\n",
        "* Flattening observation space\n",
        "* increasing learning rates\n",
        "* decreasing vf_coef\n",
        "\n",
        "# **v2.2**\n",
        " **Added:**\n",
        " * normalising values. it is time to do this.....\n",
        "\n",
        "# **v2.1**\n",
        "\n",
        "**Added:**\n",
        " * changed reward structure to give a penalty every time the agent reaches a new max demand.\n",
        " * changed obs to include max_demand\n",
        "\n",
        "# **v2.0**\n",
        "*\n",
        "Adding a continous action space! Yolo\n",
        "\n",
        "# **v1.2**\n",
        "**Added:**\n",
        "* Diesel Generator action to mitigate unmet-load\n",
        "* reward based off real prices + demand charge - Demand charge has fucked the agent cause it buys in bulk! - might be time for the continous action space so it can decide how much to buy......\n",
        "\n",
        "**To Do:**\n",
        "* impliment a priority load - not gonna do this, just gonna have unmet load\n",
        "\n",
        "* Add in actual predictions, eish\n",
        "\n",
        "* figure out how to have more episodes!! I think it will have big performance boosts on the training :)\n",
        "\n",
        "* thinking that maybe I should change obs space back to what it was cause I dont actully need to know individual loads and gens!!!!\n",
        "\n",
        "# **v1.1**\n",
        "\n",
        "**Added:**\n",
        "* rect and inverter power tracking\n",
        "* reward logging in my own logging func\n",
        "* changed logging vars to arrays\n",
        "*\n",
        "\n",
        "**To Do**\n",
        "\n",
        "* battery charging rates - I think my assumption is fine.\n",
        "\n",
        "* tweak visualisation to show bar graphs at the end of training/testing. Maybe just print graphs at the end? I have added plt.show() - remember to play if it doesnt work!\n",
        "\n",
        "* impliment a generator!!!!!\n",
        "* impliment a priority load\n",
        "\n",
        "* NB figure out how to have more episodes!! I think it will have big performance boosts on the training :)\n",
        "\n",
        "* thinking that maybe I should change obs space back to what it was cause I dont actully need to know individual loads and gens!!!!\n",
        "\n",
        "# **v1.0**\n",
        "\n",
        "**Added:**\n",
        "* AC and DC load\n",
        "* Wind Gen\n",
        "* changed obs space to hold new loads\n",
        "* re wrote standby and purchase functions\n",
        "\n",
        "**To DO**\n",
        "* thinking that maybe I should change obs space back to what it was cause I dont actully need to know individual loads and gens!!!!\n",
        "* Add in rectifier & inverter power tracking\n",
        "* battery charging rates\n",
        "\n",
        "\n",
        "# **v0.3**\n",
        "**Added:**\n",
        "* loadshedding\n",
        "* New reward structure\n",
        "* added Vec_env\n",
        "* added eval callbacks to validate training\n",
        "* added in logging\n",
        "\n",
        "**Parameters:**\n",
        "* Added in loadshedding forecast\n",
        "\n",
        "**To do:**\n",
        "* find out about battery charging rates  & impliment\n",
        "* Find out if I need to normalise\n",
        "* Try dqn\n",
        "* Find out how the bounds for the obs_space box effect things\n",
        "* Try play with DummyVec wrapper (didnt work in last version)\n",
        "\n",
        "# **v0.2_1**\n",
        "**Added:**\n",
        "*  Monitor wrapper\n",
        "*  DummyVec wrapper\n",
        "*  Wand (weights and bais) enabled\n",
        "\n",
        "**Parameters:**\n",
        "* lowered to 3 predictions  \n",
        "\n",
        "**To do:**\n",
        "* Try to use hyperparameter Optimisation - decided Im only going to do on the final version\n",
        "* Try normalise\n",
        "* Try differnet models\n",
        "* Find out how the bounds for the obs_space box effect things\n",
        "\n",
        "# **v0.2**\n",
        "**Added:**\n",
        "*  simplified load_forecast and gen_forecast to be power_bal_forecasts.\n",
        "*  combined current_load and current_gen to also show current_power_balance\n",
        "*  added proper evaluate call\n",
        "\n",
        "**Parameters:**\n",
        "* No changes  \n",
        "\n",
        "**Results:**\n",
        "* 5% savings on PPO deterministic = true\n",
        "* 4.3% savings on PPO determnistic = false\n",
        "\n",
        "**To do:**\n",
        "* Try to use hyperparameter Optimisation\n",
        "* Try normalise\n",
        "* Try differnet models\n",
        "* Try see if discount rate can be tweaked - at good level.\n",
        "* Find out how the bounds for the obs_space box effect things\n",
        "\n",
        "# **v0.1**\n",
        "**Added:**\n",
        "* added real loads, gen, tou_id's\n",
        "\n",
        "**Parameters:**\n",
        "* training episode = 6000 timesteps\n",
        "* testing episode  = 2760 timesteps\n",
        "* bat_threshold = 100\n",
        "* bat_cap = 500\n",
        "* battery_level at reset = bat_cap/2\n",
        "* num_preds = 24\n",
        "* Trained PPO for 1.65mil timesteps\n",
        "* Trained A2C for 1.2 mil timesteps\n",
        "\n",
        "**Results:**\n",
        "* PP0 - 3.7% improvement from standby mode Deterministic = False\n",
        "* PPO - 6.1% Deterministic =  True\n",
        "* A2C  - -0.3% improvement. And the models after this got worse as training progressed!\n",
        "**To do:**\n",
        "* Try lower num_preds\n",
        "* Try to use hyperparameter Optimisation\n",
        "* Try normalise\n",
        "* Try differnet models\n",
        "* Try see if discount rate can be tweaked\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5DZFnZnzStt1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI52iVVCCPaf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#install dependancies\n",
        "!pip install gymnasium\n",
        "!pip install stable_baselines3[extra]\n",
        "!pip install wandb\n",
        "!pip install sklearn\n",
        "%load_ext tensorboard\n",
        "\n",
        "#clone repository\n",
        "! git clone https://github.com/Julian-Banks/EEE4022S_BNKJUL001_Thesis\n",
        "\n",
        "#to update the rep\n",
        "%cd /content/EEE4022S_BNKJUL001_Thesis\n",
        "! git pull\n",
        "#import needed libarys\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from gymnasium import spaces\n",
        "import datetime\n",
        "from stable_baselines3 import PPO, A2C, DQN,DDPG\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from gym.wrappers import FlattenObservation\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "\n",
        "\n",
        "#mount the drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define paths to logs and model saves\n",
        "model_type = \"PPO\"\n",
        "version    = \"EMSv2_3\"\n",
        "model_dir = f\"/content/drive/MyDrive/Colab Notebooks/{version}/models/{model_type}/\"\n",
        "log_dir   = f\"/content/drive/MyDrive/Colab Notebooks/{version}/models/{model_type}logs/\"\n",
        "animation_dir = f\"/content/drive/MyDrive/Colab Notebooks/{version}/models/{model_type}/animation/\"\n",
        "\n",
        "#make the appropriate directory if it does not exist\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "if not os.path.exists(animation_dir):\n",
        "    os.makedirs(animation_dir)"
      ],
      "metadata": {
        "id": "SIpnNCVVRTV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define our environment class!!**"
      ],
      "metadata": {
        "id": "Q99_jLMvwuZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2iW-k26FIbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb7c156-e55c-45be-a741-658f3806815b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment EMSv2_3 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:428: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "class EMSv2_3(gym.Env):\n",
        "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\",\"rgb_array\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self,bat_threshold = 0.1, bat_cap = 1, actual_load = \"none\", actual_gen = \"none\", purchase_price = [1,1,1,1,1,1,1,1,2,2,2,2] , episode_len = 8760,num_preds = 24,render_mode = \"rgb_array\", load_shedding = \"none\", wandb_log = False,train_log = True, gen_size = 100,demand_charge = 252.92):\n",
        "\n",
        "        super(EMSv2_3, self).__init__()\n",
        "        #define render_mode\n",
        "        self.render_mode = render_mode\n",
        "        #define wandb\n",
        "        self.wandb_log = wandb_log\n",
        "        self.train_log = train_log\n",
        "        #define time frame\n",
        "        self.current_step = 0\n",
        "        self.final_step = int(episode_len)-num_preds-2\n",
        "\n",
        "        #Might make a function for these\n",
        "        #fill all of the actual loads NB!!! is just random for now NB!!! is normalised 0-1\n",
        "        if isinstance(actual_load,str) :\n",
        "            self.actual_load = np.random.rand(self.final_step+num_preds+1,2).astype(np.float32) #will load from a file or something\n",
        "        else:\n",
        "            self.actual_load  = actual_load[:episode_len,:]\n",
        "\n",
        "        self.load_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "        self.actual_load = self.load_scaler.fit_transform(self.actual_load)\n",
        "\n",
        "\n",
        "\n",
        "        #fill all of the actual generation steps.\n",
        "        if isinstance(actual_gen,str):\n",
        "            self.actual_gen  = np.random.rand(self.final_step+num_preds+1,2).astype(np.float32) #will load from file or something\n",
        "        else:\n",
        "            self.actual_gen  = actual_gen[:episode_len,:]\n",
        "\n",
        "        self.gen_scaler = MinMaxScaler(feature_range = (0,1))\n",
        "        self.actual_gen = self.gen_scaler.fit_transform(self.actual_gen)\n",
        "\n",
        "        #Fill the loadShedding indicator\n",
        "        if isinstance(load_shedding,str):\n",
        "            num_shedding   = np.random.randint(int(0.02*episode_len), int(0.05*episode_len))\n",
        "            load_shed      = np.array([1]*num_shedding + [0]*(episode_len - num_shedding))\n",
        "            np.random.shuffle(load_shed)\n",
        "            self.load_shed = load_shed\n",
        "        else:\n",
        "            self.load_shed = load_shedding[:episode_len]\n",
        "\n",
        "        #define vars for render\n",
        "        self.off_peak_purchases = np.zeros(self.final_step)\n",
        "        self.standard_purchases = np.zeros(self.final_step)\n",
        "        self.peak_purchases     = np.zeros(self.final_step)\n",
        "\n",
        "        self.off_peak_num       = 0\n",
        "        self.peak_num           = 0\n",
        "        self.standard           = 0\n",
        "\n",
        "        self.unmet_load_total   = 0\n",
        "        self.frames = []\n",
        "\n",
        "        #Define a var for unmet load no that there is loadshedding\n",
        "        self.step_unmet_load = np.zeros(self.final_step)\n",
        "\n",
        "        #define the purchase price for every step of the year\n",
        "        purchase_price = np.array(purchase_price).astype(np.float32)\n",
        "        repetitions    = (self.final_step+num_preds+1) // len(purchase_price)\n",
        "        remainder      = (self.final_step+num_preds+1) % len(purchase_price)\n",
        "        self.purchase_price =np.concatenate([purchase_price]*repetitions+[purchase_price[:remainder]])#need to read in from somewhere\n",
        "        price_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "        self.purchase_price = price_scaler.fit_transform(self.purchase_price.reshape(-1, 1))\n",
        "        self.price_scaler = price_scaler\n",
        "        #define demand charge\n",
        "        self.demand_charge = demand_charge\n",
        "        #define var for storing the excess gen\n",
        "        self.excess_gen = np.zeros(self.final_step)\n",
        "        #define the size of the diesel_gen\n",
        "        self.gen_size = gen_size\n",
        "        #define a var for determine amount purchased per step (dont want to make it total as this will incure growing penalties for the Agent if used in reward structure)\n",
        "        self.step_purchased = np.zeros(self.final_step)\n",
        "        self.purchased_total = 0\n",
        "        #define the battery max capacity\n",
        "        self.bat_cap = bat_cap\n",
        "        #define the battery low threshold\n",
        "        self.bat_threshold = np.float32(bat_threshold)\n",
        "\n",
        "        bat_scaler = MinMaxScaler(feature_range = (0,1))\n",
        "        [bat_threshold_norm, bat_cap_norm] = bat_scaler.fit_transform(np.array([np.float32(bat_threshold), np.float32(bat_cap)]).reshape(-1,1))\n",
        "        self.bat_scaler = bat_scaler\n",
        "        self.battery_level = np.zeros(self.final_step+1)\n",
        "\n",
        "\n",
        "        self.action_scaler = MinMaxScaler(feature_range = (0,1))\n",
        "        self.action_scaler.fit_transform(np.array([0, self.bat_cap -  self.bat_threshold + np.max(actual_load))]).reshape(-1,1))\n",
        "\n",
        "        self.num_preds = num_preds # day ahead predictions\n",
        "        #define how many different loads and generators there are\n",
        "        self.num_loads = self.actual_load.shape[1]\n",
        "        #define the size of the action space\n",
        "        self.action_space = gym.spaces.Box(low= 0 , high =1)\n",
        "        # Dict space to store all the different things\n",
        "\n",
        "        self.observation_space = spaces.Dict({\n",
        "                \"power_bal_forecast\" : gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1,num_preds), dtype=np.float32),\n",
        "                \"price_forecast\"     : gym.spaces.Box(low=0, high=np.inf, shape=(1,num_preds+1), dtype=np.float32),\n",
        "                \"island_forecast\"    : gym.spaces.Box(low=0, high=1, shape=(1,num_preds+1), dtype=np.float32),\n",
        "                \"bat_level\"          : gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
        "                \"current_power_bal\"  : gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
        "                })\n",
        "        self.observation_space = spaces.flatten_space(self.observation_space)\n",
        "        #\"max_demand\"         : gym.spaces.Box(low=0, high = np.inf, shape = (1,),dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        #update the current state with the action (needs to be done before current_step is inc since we want to apply the action to the previous step to get the current state)\n",
        "        self.update_state(action)\n",
        "        #Calculate reward from the action\n",
        "        self.reward[self.current_step] = self.calc_reward()\n",
        "        #print(f\"calc_reward: {self.calc_reward()}\")\n",
        "        #print(f\"self.reward[current_step]: {self.reward[self.current_step]}\")\n",
        "        reward = self.reward[self.current_step]\n",
        "        #inc time step into Future\n",
        "        if self.wandb_log == True:\n",
        "            #doing this for training logging\n",
        "            if self.train_log == True:\n",
        "                if self.current_step == self.final_step:\n",
        "                    self.wandb_logger()\n",
        "            else:\n",
        "                self.wandb_logger()\n",
        "\n",
        "        self.current_step += 1\n",
        "        #get next observation (for next time step)\n",
        "        observation = self.get_obs()\n",
        "        #Set terminated to False since there are no failure states\n",
        "        self.terminated = False\n",
        "        #Check if timelimit reached\n",
        "        self.truncated = False if self.current_step<self.final_step else True\n",
        "        #Wand log, if its set to true(so that it only gets run when wandb is initialised)\n",
        "        if self.wandb_log == True:\n",
        "            #doing this for training logging\n",
        "            if self.train_log == True:\n",
        "                if self.current_step == self.final_step:\n",
        "                    self.wandb_logger()\n",
        "            else:\n",
        "                self.wandb_logger()\n",
        "        #dont know what to put into info for now\n",
        "        info = {}\n",
        "        return observation, reward, self.terminated, self.truncated, info\n",
        "'''\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed = seed, options=options)\n",
        "        self.current_step = 0\n",
        "        self.terminated = False\n",
        "        self.truncated = False\n",
        "        #reset the state\n",
        "        self.battery_level[0]   = self.bat_scaler.transform(np.array(self.bat_cap/2).reshape(-1,1))\n",
        "        self.excess_gen         = np.zeros(self.final_step+1)\n",
        "        self.step_purchased     = np.zeros(self.final_step+1)\n",
        "        self.step_unmet_load    = np.zeros(self.final_step+1)\n",
        "        self.off_peak_purchases = np.zeros(self.final_step+1)\n",
        "        self.peak_purchases     = np.zeros(self.final_step+1)\n",
        "        self.standard_purchases = np.zeros(self.final_step+1)\n",
        "        self.diesel_count       = np.zeros(self.final_step+1)\n",
        "        self.reward             = np.zeros(self.final_step+1)\n",
        "        self.step_invt          = np.zeros(self.final_step+1)\n",
        "        self.step_rect          = np.zeros(self.final_step+1)\n",
        "        self.diesel_gen         = np.zeros(self.final_step+1)\n",
        "        self.action_purchase    = np.zeros(self.final_step+1)\n",
        "        self.money_spent        = np.zeros(self.final_step+1)\n",
        "\n",
        "        #get the first observation\n",
        "        observation = self.get_obs()\n",
        "        #Still don't know what to do with info\n",
        "        info = {}\n",
        "        return observation, info\n",
        "'''\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed = seed, options=options)\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "\n",
        "        self.current_step = random.randint(0, self.final_step-30*24)\n",
        "        self.final_step   = self.current_step + 30*24\n",
        "        self.terminated = False\n",
        "        self.truncated = False\n",
        "        #reset the state\n",
        "        self.battery_level[0]   = self.bat_scaler.transform(np.array(self.bat_cap/2).reshape(-1,1))\n",
        "        self.excess_gen         = np.zeros(self.final_step+1)\n",
        "        self.step_purchased     = np.zeros(self.final_step+1)\n",
        "        self.step_unmet_load    = np.zeros(self.final_step+1)\n",
        "        self.off_peak_purchases = np.zeros(self.final_step+1)\n",
        "        self.peak_purchases     = np.zeros(self.final_step+1)\n",
        "        self.standard_purchases = np.zeros(self.final_step+1)\n",
        "        self.diesel_count       = np.zeros(self.final_step+1)\n",
        "        self.reward             = np.zeros(self.final_step+1)\n",
        "        self.step_invt          = np.zeros(self.final_step+1)\n",
        "        self.step_rect          = np.zeros(self.final_step+1)\n",
        "        self.diesel_gen         = np.zeros(self.final_step+1)\n",
        "        self.action_purchase    = np.zeros(self.final_step+1)\n",
        "        self.money_spent        = np.zeros(self.final_step+1)\n",
        "\n",
        "        #get the first observation\n",
        "        observation = self.get_obs()\n",
        "        #Still don't know what to do with info\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "\n",
        "    def render(self, mode='rgb_array', save_path=None):\n",
        "\n",
        "        plt.clf()\n",
        "        values = [self.off_peak_purchases, self.standard_purchases, self.purchase_price[self.peak_purchases]]\n",
        "        colors = ['green', 'orange','red']\n",
        "        labels = ['Off Peak', 'Standard', 'Peak']\n",
        "        plt.xlim(0,1.6)\n",
        "        plt.ylim(0,100)\n",
        "        plt.bar(list(range(3)),values, color=colors, tick_label=labels)\n",
        "        self.frames.append(plt.gcf().canvas.tostring_rgb())\n",
        "        plt.pause(0.000001)\n",
        "\n",
        "    def wandb_logger(self):\n",
        "        bat_scaler = self.bat_scaler\n",
        "        load_scaler = self.load_scaler\n",
        "        gen_scaler = self.gen_scaler\n",
        "        train_log_dict={\n",
        "                    \"Excess Generation\"         :np.sum(self.excess_gen),\n",
        "                    \"Unmet Load\"                :np.sum(self.step_unmet_load),\n",
        "                    \"Off-Peak Purchases\"        :np.sum(self.off_peak_purchases),\n",
        "                    \"Standard Purchases\"        :np.sum(self.standard_purchases),\n",
        "                    \"Peak Purchases\"            :np.sum(self.peak_purchases),\n",
        "                    \"Num Off-Peak Purchases\"    :np.count_nonzero(self.off_peak_purchases),\n",
        "                    \"Num Standard Purchases\"    :np.count_nonzero(self.standard_purchases),\n",
        "                    \"Num Peak Purchases\"        :np.count_nonzero(self.peak_purchases),\n",
        "                    \"Total Elec Purchase\"       :np.sum(self.step_purchased),\n",
        "                    \"Total Purchase Requested\"  :np.sum(self.action_purchase),\n",
        "                    \"Num Diesel Gen Actions\"    :np.count_nonzero(self.diesel_count),\n",
        "                    \"Total Reward\"              :np.sum(self.reward),\n",
        "                    \"Total Money spent\"         :np.sum(self.money_spent),\n",
        "                    \"Rectifier total power flow\":np.sum(self.step_rect),\n",
        "                    \"Inverter total power flow\" :np.sum(self.step_invt),\n",
        "                    \"Diesel Generator\"          :np.sum(self.diesel_gen),\n",
        "                    \"Max Demand\"                :np.max(self.step_purchased),\n",
        "                    \"total Purchased Elec\"      :np.sum(self.step_purchased)\n",
        "\n",
        "\n",
        "                  }\n",
        "        load = load_scaler.inverse_transform(self.actual_load[self.current_step,:].reshape(-1,2)).flatten()\n",
        "        gen  = gen_scaler.inverse_transform(self.actual_gen[self.current_step,:].reshape(-1,2)).flatten()\n",
        "        eval_log_dict={\n",
        "                    \"battery_level\"             :bat_scaler.inverse_transform(self.battery_level[self.current_step].reshape(-1,1)).flatten(),\n",
        "                    \"AC load\"                   :load[0],\n",
        "                    \"DC load\"                   :load[1],\n",
        "                    \"Wind generation\"           :gen[0],\n",
        "                    \"PV generation\"             :gen[1],\n",
        "                    \"Excess Generation\"         :self.excess_gen[self.current_step],\n",
        "                    \"Unmet Load\"                :self.step_unmet_load[self.current_step],\n",
        "                    \"LoadShedding\"              :self.load_shed[self.current_step],\n",
        "                    \"Off-Peak Purchases\"        :self.off_peak_purchases[self.current_step],\n",
        "                    \"Standard Purchases\"        :self.standard_purchases[self.current_step],\n",
        "                    \"Peak Purchases\"            :self.peak_purchases[self.current_step],\n",
        "                    \"Num Off-Peak Purchases\"    :np.count_nonzero(self.off_peak_purchases),\n",
        "                    \"Num Standard Purchases\"    :np.count_nonzero(self.standard_purchases),\n",
        "                    \"Num Peak Purchases\"        :np.count_nonzero(self.peak_purchases),\n",
        "                    \"Step Purchased\"            :self.step_purchased[self.current_step],\n",
        "                    \"Purchase Requested\"        :self.action_purchase[self.current_step],\n",
        "                    \"Num Diesel Gen Actions\"    :np.count_nonzero(self.diesel_count),\n",
        "                    \"Total Reward\"              :self.reward[self.current_step],\n",
        "                    \"Money Spent\"               :self.money_spent[self.current_step],\n",
        "                    \"Rectifier total power flow\":self.step_rect[self.current_step],\n",
        "                    \"Inverter total power flow\" :self.step_invt[self.current_step],\n",
        "                    \"Diesel Generator\"          :self.diesel_gen[self.current_step],\n",
        "\n",
        "                  }\n",
        "\n",
        "        if self.train_log:\n",
        "            wandb.log(train_log_dict)\n",
        "        else:\n",
        "            wandb.log(eval_log_dict)\n",
        "\n",
        "        if self.train_log == False and self.current_step == self.final_step:\n",
        "\n",
        "\n",
        "            values = [[np.sum(self.off_peak_purchases),'Off Peak'], [np.sum(self.standard_purchases),'Standard'], [np.sum(self.peak_purchases),'Peak']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            Tou_purchases = wandb.plot.bar(table,\"labels\",\"values\", title=\"kWh purchased per TOU tariff\")\n",
        "\n",
        "            values = [[np.max(self.step_purchased),'Max demand'], [np.max(self.diesel_gen),'Max diesel gen'], [np.max(self.step_rect),'Max rectifier power'], [np.max(self.step_invt),'Max inverter power']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            max_metrics = wandb.plot.bar(table,\"labels\",\"values\", title=\"Sizing Metrics\")\n",
        "\n",
        "            values = [[np.sum(self.step_purchased),'Total Purchased'], [np.sum(self.action_purchase),'Total requested purchases'],[np.sum(self.excess_gen),'Total excess generation']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            total_metrics = wandb.plot.bar(table,\"labels\",\"values\", title=\"Total Metrics\")\n",
        "\n",
        "            values = [[np.sum(self.diesel_gen),'Total diesel generation'], [np.sum(self.step_unmet_load),'Total unmet load']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            total_diesel_unmet = wandb.plot.bar(table,\"labels\",\"values\", title=\"Total diesel and unmet load\")\n",
        "\n",
        "            values = [[np.sum(self.reward),'Total reward accumulated'], [np.sum(self.money_spent),'Total money spent']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            total_reward = wandb.plot.bar(table,\"labels\",\"values\", title=\"Total reward and money spent\")\n",
        "\n",
        "\n",
        "            wandb.log({ \"kWh purchased per TOU tariff\"  : Tou_purchases,\n",
        "                        \"Max sizing metrics\"            : max_metrics,\n",
        "                        \"Total metrics\"                 : total_metrics,\n",
        "                        \"Total diesel and unmet load\"   : total_diesel_unmet,\n",
        "                        \"Total reward and money spent\"  :total_reward\n",
        "                        })\n",
        "\n",
        "\n",
        "            '''\n",
        "            plt.clf()\n",
        "            values = [np.sum(self.off_peak_purchases), np.sum(self.standard_purchases), np.sum(self.peak_purchases)]\n",
        "            colors = ['green', 'orange','red']\n",
        "            plt.xlim(0,2)\n",
        "            plt.ylim(0,120000)\n",
        "            plt.bar(list(range(3)),values, color=colors, tick_label=labels)\n",
        "            plt.xlabel('Tariff Rate')\n",
        "            plt.ylabel('Electricity Purchased (units)')\n",
        "            plt.title('Electricity Purchased per Tariff Rate')\n",
        "\n",
        "            save_name = animation_dir + \"TOU_purchases_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\".png\"\n",
        "            plt.savefig(save_name)\n",
        "            wandb.log({\"Electricity purchased per Tariff rate\": wandb.Image(save_name)})\n",
        "            '''\n",
        "    def close(self):\n",
        "        #don't think i need this for my application\n",
        "        pass\n",
        "\n",
        "    def update_state(self, action):\n",
        "\n",
        "        self.standby(purchase_amount = action)\n",
        "        self.action_purchase[self.current_step] = action\n",
        "\n",
        "        self.tou_purchase_inc()\n",
        "        #Calculate the flow of power (this is max absorb, the min need is just dc_power_bal)\n",
        "        #fetch info from grids\n",
        "        ac_power_bal, avail_grid = self.AC_bus()\n",
        "        dc_power_bal, avail_bat, avail_stor = self.DC_bus()\n",
        "        self.calc_power_flow(dc_power_bal,ac_power_bal,avail_stor)\n",
        "\n",
        "    def calc_reward(self):\n",
        "        #Calculate reward based on the state\n",
        "        #Summer Months: 5.92, 2.09, 1.33\n",
        "        #Winter Months: 2.22, 1.66, 1.21\n",
        "        #Demand Charge: 252.92\n",
        "\n",
        "        petrol_per_kw  = 0.4*23\n",
        "        diesel_cost     = petrol_per_kw*self.diesel_gen[self.current_step] #0.4l per kwh produced multipled by a cost of 23 rand per litre. Very rough values\n",
        "        elec_purchase   = self.step_purchased[self.current_step]*self.price_scaler.inverse_transform(self.purchase_price[self.current_step].reshape(-1,1)).flatten()\n",
        "        if self.off_peak_purchases[self.current_step] >0 :\n",
        "            bonus = self.step_purchased[self.current_step]*0.8\n",
        "        else:\n",
        "            bonus = 0\n",
        "\n",
        "        unmet_load_pen  = self.step_unmet_load[self.current_step]*20\n",
        "\n",
        "        if self.current_step > 30*24:\n",
        "            start_index = self.current_step - self.current_step % (24*30)\n",
        "            current_max  = np.max(self.step_purchased[start_index:])\n",
        "        elif self.current_step> 0:\n",
        "            current_max = np.max(self.step_purchased[:self.current_step])\n",
        "        else:\n",
        "            current_max = self.step_purchased[self.current_step]\n",
        "\n",
        "        if self.step_purchased[self.current_step] > current_max:\n",
        "            max_demand_pen  = (self.step_purchased[self.current_step] - current_max)*self.demand_charge\n",
        "        else:\n",
        "            max_demand_pen = 0\n",
        "\n",
        "        reward = -elec_purchase - unmet_load_pen - diesel_cost #- max_demand_pen + bonus\n",
        "\n",
        "        self.money_spent[self.current_step] =  elec_purchase +  diesel_cost\n",
        "        #if a month has past, then impliment the demand charge\n",
        "        if self.current_step % (24*30) == 0 and self.current_step != 0:\n",
        "            max_grid_demand = np.max(self.step_purchased[self.current_step-(24*30):self.current_step])\n",
        "            reward = reward #- max_grid_demand * self.demand_charge\n",
        "            self.money_spent[self.current_step] =  self.money_spent[self.current_step] + max_grid_demand*self.demand_charge\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_obs(self):\n",
        "        #Fill the observation space with the next observation\n",
        "\n",
        "        #Get Forecasts Will probaly write a function for this? idk maybe a schlep to return all the info\n",
        "        load_forecast  = np.array( [self.actual_load[self.current_step+1: self.current_step + self.num_preds+1,:]] , dtype = np.float32) #will load from a file or something\n",
        "        gen_forecast   = np.array( [self.actual_gen[self.current_step+1: self.current_step + self.num_preds+1,:]] , dtype = np.float32) #will load from a file or something\n",
        "        #calculate the power forecast\n",
        "        power_bal_forecast = gen_forecast-load_forecast\n",
        "        power_bal_forecast = power_bal_forecast.sum(axis=2)\n",
        "        #get the prices for the current frame and the next 24 hours. Maybe will cut this down since that seems like a lot of info\n",
        "        price_forecast = np.array( [self.purchase_price[self.current_step:self.current_step+self.num_preds+1]] , dtype = np.float32).reshape(1,self.num_preds+1)\n",
        "        #Just for readibility of the dict object\n",
        "        bat_level      = np.array([self.battery_level[self.current_step]] , dtype= np.float32)\n",
        "        #island forecasst, same as tou forecast\n",
        "        island_forecast =np.array( [self.load_shed[self.current_step:self.current_step+self.num_preds+1]] , dtype = np.float32)\n",
        "\n",
        "        #calculate the current power balance\n",
        "        current_load   = np.array([self.actual_load[self.current_step,:]], dtype = np.float32)\n",
        "        current_gen    = np.array([self.actual_gen[self.current_step,:]], dtype  = np.float32)\n",
        "        current_power_bal = current_gen - current_load\n",
        "        current_power_bal = current_power_bal.sum(axis=1)\n",
        "        '''\n",
        "        if self.current_step>30*24:\n",
        "            max_demand = np.array([np.max(self.step_purchased[self.current_step-(24*30):self.current_step])],dtype = np.float32)\n",
        "        else:\n",
        "            max_demand = np.array([np.max(self.step_purchased)],dtype = np.float32)\n",
        "        '''\n",
        "        if self.current_step > 30*24:\n",
        "            start_index = self.current_step - self.current_step % (24*30)\n",
        "            current_max  = np.max(self.step_purchased[start_index:])\n",
        "        elif self.current_step> 0:\n",
        "            current_max = np.max(self.step_purchased[:self.current_step])\n",
        "        else:\n",
        "            current_max = self.step_purchased[self.current_step]\n",
        "\n",
        "        if self.step_purchased[self.current_step] > current_max:\n",
        "            max_demand  = (self.step_purchased[self.current_step-1] - current_max)\n",
        "            max_demand  = np.array([max_demand],dtype = np.float32)\n",
        "        else:\n",
        "            max_demand  = np.array([0],dtype = np.float32)\n",
        "\n",
        "        # \"max_demand\"    : max_demand\n",
        "        obs = dict({\n",
        "                \"bat_level\":      bat_level,\n",
        "                \"current_power_bal\" :   current_power_bal,\n",
        "                \"island_forecast\": island_forecast,\n",
        "                \"power_bal_forecast\":  power_bal_forecast,\n",
        "                \"price_forecast\": price_forecast,\n",
        "\n",
        "        })\n",
        "        obs = np.concatenate([obs[key].flatten() for key in obs.keys()])\n",
        "        return obs\n",
        "\n",
        "    def AC_bus(self):\n",
        "        #fill out info on the ac\n",
        "        ac_gen = self.gen_scaler.inverse_transform((self.actual_gen[self.current_step, :]).reshape(-1,2)).flatten()\n",
        "        ac_gen = ac_gen[0]\n",
        "        ac_load = self.load_scaler.inverse_transform(self.actual_load[self.current_step,:].reshape(-1,2)).flatten()\n",
        "        ac_load = ac_load[0]\n",
        "        ac_power_bal = ac_gen - ac_load\n",
        "        #check if there is load shedding or not\n",
        "        avail_grid = not self.load_shed[self.current_step]\n",
        "        #ac_diesel = Don't know what yet but I do want to use it for something.\n",
        "        #return relevant values\n",
        "        return ac_power_bal,avail_grid\n",
        "\n",
        "    def DC_bus(self):\n",
        "        #fill in info for DC_bus\n",
        "        dc_gen       = self.gen_scaler.inverse_transform((self.actual_gen[self.current_step,:]).reshape(-1,2)).flatten()\n",
        "        dc_gen  = dc_gen[1]\n",
        "        dc_load      = self.load_scaler.inverse_transform((self.actual_load[self.current_step,:]).reshape(-1,2)).flatten()\n",
        "        dc_load = dc_load[1]\n",
        "        dc_power_bal = dc_gen - dc_load\n",
        "        #Haven't imposed limits here but I don't think I need to. Must check.\n",
        "        avail_bat  = self.bat_scaler.inverse_transform(self.battery_level[self.current_step].reshape(-1,1)) - self.bat_threshold\n",
        "        avail_stor = self.bat_cap   - self.bat_scaler.inverse_transform(self.battery_level[self.current_step].reshape(-1,1))\n",
        "        return dc_power_bal, avail_bat, avail_stor\n",
        "\n",
        "    def standby(self,purchase_amount):\n",
        "        #fetch info from grids\n",
        "        ac_power_bal, avail_grid = self.AC_bus()\n",
        "        dc_power_bal, avail_bat, avail_stor = self.DC_bus()\n",
        "        current_battery_level =  self.bat_scaler.inverse_transform(self.battery_level[self.current_step].reshape(-1,1))\n",
        "\n",
        "        purchase_amount_unnorm = self.action_scaler.transform(np.array(purchase_amount).reshape(-1,1))\n",
        "        if avail_grid: # if there is no loadshedding purchase the amount requested by the agent.\n",
        "            self.step_purchased[self.current_step] = purchase_amount_unnorm\n",
        "        else:\n",
        "            self.step_purchased[self.current_step] = 0\n",
        "        #calculate the immediate power_bal\n",
        "        grid_power_bal = ac_power_bal + dc_power_bal + self.step_purchased[self.current_step]\n",
        "        #determine the flow of power:\n",
        "        if grid_power_bal > 0 :\n",
        "            #increments the battery level by the minimium between avail_stor and grid_power_bal ( always keeps it in range)\n",
        "            self.battery_level[self.current_step+1] =  self.bat_scaler.transform(current_battery_level+ min(avail_stor, grid_power_bal))\n",
        "            #increments excess gen by the max ( if grid_power_bal - avail_stor is negative, there was no excess and it will add 0, else it will add the excess that wasn't stored)\n",
        "            self.excess_gen[self.current_step] = max((grid_power_bal - avail_stor), 0)\n",
        "        else:\n",
        "            #there is a shortage of power, see if we can take it from the battery.\n",
        "            # flipping the sign of the avail bat, cause that will subtracted from current balance.\n",
        "            self.battery_level[self.current_step+1] = self.bat_scaler.transform(current_battery_level + max(-avail_bat, grid_power_bal))\n",
        "            #check if we are islanded and buy elec if we arent\n",
        "            if avail_grid:\n",
        "                #if the grid is available\n",
        "                #the min power needed is 0 if the grid power bal is positive with the available batter. Otherwise it is the shortage (grid_power_bal is negative, avail_bat is positive)\n",
        "                min_power_need = - min(grid_power_bal + avail_bat, 0)\n",
        "                self.step_purchased[self.current_step] = purchase_amount_unnorm + min_power_need\n",
        "            else:\n",
        "                #if not available add to the step un_met_load.\n",
        "                unmet_load = -min(grid_power_bal + avail_bat, 0)\n",
        "                self.diesel_gen[self.current_step] = min(unmet_load,self.gen_size)\n",
        "                if self.diesel_gen[self.current_step] > 0:\n",
        "                    self.diesel_count[self.current_step] =1\n",
        "                self.step_unmet_load[self.current_step] = max(unmet_load - self.gen_size, 0)\n",
        "\n",
        "\n",
        "    def calc_power_flow(self,dc_power_bal,ac_power_bal,avail_stor):\n",
        "\n",
        "        #Calculate the flow of power (this is max absorb, the min need is just dc_power_bal)\n",
        "        dc_power_absorb = max(-dc_power_bal+avail_stor,0)\n",
        "        #ac power excess will be the power balance added to the purchase amount\n",
        "        ac_power_excess = max(ac_power_bal+self.step_purchased[self.current_step]+self.diesel_gen[self.current_step],0)\n",
        "        #the power that will flow through the rectifier is the minimum between the amount the DC grid can absorb and the excess the ac_grid has\n",
        "        rect_power = min(dc_power_absorb,ac_power_excess)\n",
        "\n",
        "        dc_power_avail = max(dc_power_bal,0) #lol just defined a new var for readability.\n",
        "        ac_power_need   = max(-ac_power_bal-self.step_purchased[self.current_step]-self.diesel_gen[self.current_step], 0) # calculate how much power the ac grid needs.\n",
        "        dc_power_excess = max(dc_power_avail - ac_power_need - avail_stor,0) #calculate how much power would be in excess if there was to be excess.\n",
        "        #inverter power is equal to the minimum val between the avail dc power and the needed power. Then since all excess power needs to go to the grid, if there is any extra energy after the ac_need has been met and the battery is fully charged, it is also sent through the inverter\n",
        "        invt_power = min(ac_power_need, dc_power_avail) + dc_power_excess\n",
        "        #set the attributes.\n",
        "        self.step_invt[self.current_step] = invt_power\n",
        "        self.step_rect[self.current_step] = rect_power\n",
        "\n",
        "    def tou_purchase_inc(self):\n",
        "        #Summer Months: 5.92, 2.09, 1.33\n",
        "        #Winter Months: 2.22, 1.66, 1.21\n",
        "        step_price = self.price_scaler.inverse_transform(self.purchase_price[self.current_step].reshape(-1,1))\n",
        "\n",
        "        if self.step_purchased[self.current_step] != 0:\n",
        "            if step_price < 1.5:\n",
        "                self.off_peak_purchases[self.current_step] = self.step_purchased[self.current_step]\n",
        "            elif step_price < 2.1:\n",
        "                self.standard_purchases[self.current_step] = self.step_purchased[self.current_step]\n",
        "            else:\n",
        "                self.peak_purchases[self.current_step] = self.step_purchased[self.current_step]\n",
        "\n",
        "\n",
        "#define a pointer (kinda, don't actually know what its called) a\n",
        "EMS = EMSv2_3\n",
        "\n",
        "# Register environment so I can use make_vec_env\n",
        "register(\n",
        "# unique identifier for the env `name-version`\n",
        "id=f\"{version}\",\n",
        "# path to the class for creating the env\n",
        "# Note: entry_point also accept a class as input (and not only a string)\n",
        "entry_point= EMS(),\n",
        "\n",
        ")\n",
        "\n",
        "#Check the environment with stable_baselines3 check_env.\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "env = EMS()\n",
        "check_env(env,warn = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG4gB2cM7tDY"
      },
      "source": [
        "**Load in the data for our specific microgrid.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X9RBdXC_2PD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3c8e19-3aab-4e52-b6d4-ade91b7665ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reset observation space looks like: [ 0.375      -0.03574069  1.          0.          0.          0.\n",
            " -0.08455629 -0.07620816  0.01003803  0.          0.          0.\n",
            "  0.        ]\n",
            "After action 0: \n",
            "Battery level is: 0.375kWh\n",
            "Current  Power Balance -0.0357406884431839\n",
            "Forecasted  power bal 1 hour ahead: -0.08455628901720047.\n",
            "Forecasted  power bal 2 hour ahead: -0.07620815932750702. \n",
            "Forecasted  power bal 3 hour ahead: 0.010038033127784729. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.0. \n",
            "Forecasted  price 3 hour ahead: 0.0. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "After action 0: \n",
            "Battery level is: 0.1560579240322113kWh\n",
            "Current  Power Balance -0.08455628901720047\n",
            "Forecasted  power bal 1 hour ahead: -0.07620815932750702.\n",
            "Forecasted  power bal 2 hour ahead: 0.010038033127784729. \n",
            "Forecasted  power bal 3 hour ahead: 0.1709338277578354. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.0. \n",
            "Forecasted  price 3 hour ahead: 0.0. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "After action 0: \n",
            "Battery level is: -5.587935447692871e-09kWh\n",
            "Current  Power Balance -0.07620815932750702\n",
            "Forecasted  power bal 1 hour ahead: 0.010038033127784729.\n",
            "Forecasted  power bal 2 hour ahead: 0.1709338277578354. \n",
            "Forecasted  power bal 3 hour ahead: 0.3181580901145935. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.0. \n",
            "Forecasted  price 3 hour ahead: 0.0. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "After action 0: \n",
            "Battery level is: -5.587935447692871e-09kWh\n",
            "Current  Power Balance 0.010038033127784729\n",
            "Forecasted  power bal 1 hour ahead: 0.1709338277578354.\n",
            "Forecasted  power bal 2 hour ahead: 0.3181580901145935. \n",
            "Forecasted  power bal 3 hour ahead: 0.5346448421478271. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.0. \n",
            "Forecasted  price 3 hour ahead: 0.09576204419136047. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "After action 100: \n",
            "Battery level is: 0.08829387277364731kWh\n",
            "Current  Power Balance 0.1709338277578354\n",
            "Forecasted  power bal 1 hour ahead: 0.3181580901145935.\n",
            "Forecasted  power bal 2 hour ahead: 0.5346448421478271. \n",
            "Forecasted  power bal 3 hour ahead: 0.7436432242393494. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.09576204419136047. \n",
            "Forecasted  price 3 hour ahead: 0.21427208185195923. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "Done iteration! Total reward accumulated is: -1776756.8539136872\n"
          ]
        }
      ],
      "source": [
        "#need to import data from Github\n",
        "path_data = \"/content/EEE4022S_BNKJUL001_Thesis/PythonWorkspace/dataClean.csv\"\n",
        "data = pd.read_csv(path_data)\n",
        "\n",
        "path_pv_gen = \"/content/EEE4022S_BNKJUL001_Thesis/Generation/BNKJUL001_Thesis_solarGen500kWHomer.csv\"\n",
        "data_pv_gen = pd.read_csv(path_pv_gen)\n",
        "\n",
        "path_wind_gen = \"/content/EEE4022S_BNKJUL001_Thesis/Generation/BNKJUL001_Thesis_Wind500kGenHomer.csv\"\n",
        "data_wind_gen = pd.read_csv(path_wind_gen)\n",
        "\n",
        "#Not actually using this rn but will be soon :)\n",
        "path_shedding = \"/content/EEE4022S_BNKJUL001_Thesis/MatlabWorkSpace/loadShedding2022.csv\"\n",
        "data_shedding = pd.read_csv(path_shedding)\n",
        "load_shedding = data_shedding['LoadShedding'].values.astype(np.float32)\n",
        "\n",
        "wind_gen = data_wind_gen['Wind_Out'].values.astype(np.float32)\n",
        "PV_gen = data_pv_gen['PV_Out'].values.astype(np.float32)\n",
        "actual_gen = np.column_stack((wind_gen, PV_gen))\n",
        "#read in ac and DC load\n",
        "AC_load = data['AC'].values.astype(np.float32)\n",
        "DC_load = data['DC'].values.astype(np.float32)\n",
        "#stack em together for the input :)\n",
        "actual_load = np.column_stack((AC_load, DC_load))\n",
        "\n",
        "path_purchase_price = \"/content/EEE4022S_BNKJUL001_Thesis/PythonWorkspace/purchasePrice.csv\"\n",
        "data_purchase_price = pd.read_csv(path_purchase_price)\n",
        "purchase_price = data_purchase_price['Grid Power Price'].values.astype(np.float32)\n",
        "\n",
        "\n",
        "#define the base environment\n",
        "base_env = EMS(episode_len = 6000, actual_load = actual_load, actual_gen = actual_gen, bat_threshold = 100, bat_cap = 500, purchase_price = purchase_price,num_preds = 3)\n",
        "#going to print out a bunch of things to test the different spaces.\n",
        "obs,_    = base_env.reset()\n",
        "\n",
        "def print_obs(action_standby=0):\n",
        "    print(f\"After action {action_standby}: \" )\n",
        "    battery_level = obs[0]\n",
        "    print(f\"Battery level is: {battery_level}kWh\")\n",
        "    current_power_bal = obs[1]\n",
        "    print(f\"Current  Power Balance {current_power_bal}\")\n",
        "    power_forecast = obs[6:9]\n",
        "    print(f\"Forecasted  power bal 1 hour ahead: {power_forecast[0]}.\")\n",
        "    print(f\"Forecasted  power bal 2 hour ahead: {power_forecast[1]}. \")\n",
        "    print(f\"Forecasted  power bal 3 hour ahead: {power_forecast[2]}. \")\n",
        "    price = obs[9:]\n",
        "    print(f\"current Purchase Prcie: {price[0]}.\")\n",
        "    print(f\"Forecasted  price 1 hour ahead: {price[1]}. \")\n",
        "    print(f\"Forecasted  price 2 hour ahead: {price[2]}. \")\n",
        "    print(f\"Forecasted  price 3 hour ahead: {price[3]}. \")\n",
        "    print(f\"_________________________________________________________________________________________________________________\")\n",
        "    print(f\"\")\n",
        "\n",
        "print(f\"The reset observation space looks like: {obs}\")\n",
        "\n",
        "print_obs()\n",
        "\n",
        "action_standby = 0\n",
        "obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "\n",
        "print_obs()\n",
        "\n",
        "obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "\n",
        "print_obs()\n",
        "\n",
        "\n",
        "obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "print_obs()\n",
        "\n",
        "action_standby = 100\n",
        "obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "print_obs(action_standby)\n",
        "\n",
        "#Evaluate the base model (no EMS, just using standby mode)\n",
        "#A loop to get an average reward for the base model only perfoming the standby option\n",
        "#reset the environment and save the obs\n",
        "#going to run it 100 times to get a benchmark\n",
        "#reset score\n",
        "score = 0\n",
        "\n",
        "obs,_    = base_env.reset()\n",
        "#ensure that the exit condition is reset\n",
        "truncated = False\n",
        "#define the action to take\n",
        "action_standby = 0\n",
        "\n",
        "while not truncated:\n",
        "    obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "    score += reward\n",
        "\n",
        "print(f\"Done iteration! Total reward accumulated is: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pAj6-n04zyt"
      },
      "source": [
        "**LOAD OR MAKE MODEL HERE!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQDhUNGWeum4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bf4f04c7575f4e2190103b59884b65cd",
            "6c7dd6f6898e405ab813073950533ef0",
            "69e3459721d04ddebf2de2f3f6388336",
            "a8305dffe04f4d25a0e00e4f69075486",
            "ac5c5ef5464547fba9902cff1b9bc057",
            "9ec3c3d85e944ac98a392e99350655b5",
            "02ce55d5e261434c80ea52c600e58abb",
            "f6d36a98851c48258d905d558cf57a2a"
          ]
        },
        "outputId": "f4c6f7d6-6cac-42cd-9b69-8a3290cfdef0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:dhoy8vyt) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.245 MB of 0.245 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf4f04c7575f4e2190103b59884b65cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wobbly-cosmos-198</strong> at: <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/dhoy8vyt' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/dhoy8vyt</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231017_160124-dhoy8vyt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:dhoy8vyt). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/EEE4022S_BNKJUL001_Thesis/wandb/run-20231017_160515-4rncnijx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/4rncnijx' target=\"_blank\">lilac-violet-199</a></strong> to <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/4rncnijx' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/4rncnijx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Logging to runs/4rncnijx/EMSv2_3_PPO_train20231017-160527_0\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 474   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 21    |\n",
            "|    total_timesteps | 10240 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 439         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 46          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015723627 |\n",
            "|    clip_fraction        | 0.0937      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | -2.86e-06   |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 8.91e+06    |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | 0.00091     |\n",
            "|    std                  | 0.977       |\n",
            "|    value_loss           | 4.44e+07    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.78e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 438         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 70          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009019759 |\n",
            "|    clip_fraction        | 0.06        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.79e+06    |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00148    |\n",
            "|    std                  | 0.969       |\n",
            "|    value_loss           | 1.19e+08    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.78e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 431         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 94          |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007262933 |\n",
            "|    clip_fraction        | 0.0557      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 3.86e+07    |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00192    |\n",
            "|    std                  | 0.983       |\n",
            "|    value_loss           | 1.57e+08    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.78e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 431         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 118         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007845354 |\n",
            "|    clip_fraction        | 0.0976      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 5.8e+06     |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.000345   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 7.23e+07    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5.97e+03     |\n",
            "|    ep_rew_mean          | -1.78e+06    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 429          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 143          |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064612357 |\n",
            "|    clip_fraction        | 0.0642       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | 0.00285      |\n",
            "|    learning_rate        | 0.01         |\n",
            "|    loss                 | 5.21e+07     |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00134     |\n",
            "|    std                  | 0.998        |\n",
            "|    value_loss           | 8.52e+07     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.78e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 425         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 168         |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005804791 |\n",
            "|    clip_fraction        | 0.0554      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.43       |\n",
            "|    explained_variance   | 0.02        |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 5.35e+06    |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00195    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1.51e+08    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5.97e+03     |\n",
            "|    ep_rew_mean          | -1.78e+06    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 426          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 191          |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065133506 |\n",
            "|    clip_fraction        | 0.0667       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.0301       |\n",
            "|    learning_rate        | 0.01         |\n",
            "|    loss                 | 2.43e+07     |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00137     |\n",
            "|    std                  | 1.05         |\n",
            "|    value_loss           | 6.96e+07     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.78e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 424         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 217         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007240954 |\n",
            "|    clip_fraction        | 0.0647      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0.0115      |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 3.83e+07    |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00259    |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 1.25e+08    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.78e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 420         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 243         |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005864448 |\n",
            "|    clip_fraction        | 0.0526      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.51       |\n",
            "|    explained_variance   | 0.0515      |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 3.54e+07    |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00181    |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 1.02e+08    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5.97e+03     |\n",
            "|    ep_rew_mean          | -1.78e+06    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 420          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 267          |\n",
            "|    total_timesteps      | 112640       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044955476 |\n",
            "|    clip_fraction        | 0.0392       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.5         |\n",
            "|    explained_variance   | 0.0659       |\n",
            "|    learning_rate        | 0.01         |\n",
            "|    loss                 | 3.3e+07      |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00151     |\n",
            "|    std                  | 1.08         |\n",
            "|    value_loss           | 6.71e+07     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 5.97e+03     |\n",
            "|    ep_rew_mean          | -1.78e+06    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 419          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 292          |\n",
            "|    total_timesteps      | 122880       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060704863 |\n",
            "|    clip_fraction        | 0.0593       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.49        |\n",
            "|    explained_variance   | 0.0232       |\n",
            "|    learning_rate        | 0.01         |\n",
            "|    loss                 | 3.22e+07     |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.000894    |\n",
            "|    std                  | 1.09         |\n",
            "|    value_loss           | 1.21e+08     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.78e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 419         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 317         |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004714731 |\n",
            "|    clip_fraction        | 0.0405      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.52       |\n",
            "|    explained_variance   | 0.101       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 4.89e+07    |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00231    |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 9.88e+07    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.78e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 418         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 342         |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005930932 |\n",
            "|    clip_fraction        | 0.0546      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.57       |\n",
            "|    explained_variance   | 0.0862      |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.1e+06     |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.00326    |\n",
            "|    std                  | 1.18        |\n",
            "|    value_loss           | 6.41e+07    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.78e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 418         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 367         |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005289968 |\n",
            "|    clip_fraction        | 0.0567      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.59       |\n",
            "|    explained_variance   | 0.0269      |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 5.93e+06    |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.00292    |\n",
            "|    std                  | 1.19        |\n",
            "|    value_loss           | 1.17e+08    |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    \"policy_type\": \"MultiInputPolicy\",\n",
        "    \"total_timesteps\": 1_500_000,\n",
        "}\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"4022_intelligent_ems\",\n",
        "    config=config,\n",
        "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    save_code=True,  # optional\n",
        ")\n",
        "\n",
        "train_args = {\n",
        "                \"episode_len\"   : 6000,\n",
        "                \"actual_load\"   : actual_load,\n",
        "                \"actual_gen\"    : actual_gen,\n",
        "                \"bat_threshold\" : 100,\n",
        "                \"bat_cap\"       : 500,\n",
        "                \"purchase_price\": purchase_price,\n",
        "                \"num_preds\"     : 24,\n",
        "                \"load_shedding\" : load_shedding[2760:],\n",
        "                \"render_mode\"   : \"rgb_array\",\n",
        "                \"wandb_log\"     : True,\n",
        "                \"train_log\"     : True,\n",
        "\n",
        "                }\n",
        "\n",
        "eval_args = {\n",
        "                \"episode_len\"   :2760,\n",
        "                \"actual_load\"   :actual_load[6001:],\n",
        "                \"actual_gen\"    :actual_gen[6001:],\n",
        "                \"bat_threshold\" :100,\n",
        "                \"bat_cap\"       :500,\n",
        "                \"purchase_price\":purchase_price[6001:],\n",
        "                \"num_preds\"     :24,\n",
        "                \"load_shedding\" :load_shedding[6001:],\n",
        "                \"wandb_log\"     : True,\n",
        "                \"train_log\"     : False\n",
        "}\n",
        "\n",
        "#define 5 environments   for training and eval\n",
        "n_envs = 5\n",
        "n_eval_episodes =1\n",
        "train_env = make_vec_env(EMS, n_envs = n_envs,env_kwargs = train_args )\n",
        "\n",
        "\n",
        "eval_env = make_vec_env(EMS, n_envs = n_envs,env_kwargs = eval_args )\n",
        "\n",
        "\n",
        "wand_eval = f\"{version}_{model_type}_eval\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "wand_train = f\"{version}_{model_type}_train\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "wandb_callback = WandbCallback(\n",
        "                gradient_save_freq=100,\n",
        "                model_save_path=f\"models/{run.id}.{datetime.datetime.now()}\",\n",
        "                model_save_freq= 30000,\n",
        "                verbose=2,\n",
        "                log = \"all\",\n",
        "               )\n",
        "eval_callback = EvalCallback(eval_env,\n",
        "                             best_model_save_path = f\"{model_dir}{version}_{model_type}\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "                             log_path = wand_eval,\n",
        "                             eval_freq=300,\n",
        "                             n_eval_episodes = n_eval_episodes,\n",
        "                             deterministic = True,\n",
        "                             render = False,\n",
        "                             callback_after_eval = wandb_callback)\n",
        "\n",
        "\n",
        "model = PPO(\"MlpPolicy\",train_env, verbose = 1,learning_rate = 0.01,vf_coef = 0.2, tensorboard_log = f\"runs/{run.id}\") #log_dir\n",
        "\n",
        "model.learn(total_timesteps= config[\"total_timesteps\"],\n",
        "            tb_log_name = wand_train,\n",
        "            reset_num_timesteps=False,\n",
        "            callback = wandb_callback\n",
        "            )\n",
        "\n",
        "model.save(f\"{model_dir}{version}_{model_type}\"+datetime.datetime.now().strftime(\"%m%d-%H%M%S\"))\n",
        "run.finish()\n",
        "\n",
        "#c163c28885695cb2b0493ac455e296dcc8bc462a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a new test environment and load up the best performing model to test it.**"
      ],
      "metadata": {
        "id": "C9AGcLlsv4N7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inDHszZaxBUS"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"policy_type\": \"MultiInputPolicy\",\n",
        "    \"total_timesteps\": 2760,\n",
        "}\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"4022_intelligent_ems\",\n",
        "    config=config,\n",
        "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    save_code=True,  # optional\n",
        ")\n",
        "\n",
        "\n",
        "eval_args = {\n",
        "                \"episode_len\"   :2760,\n",
        "                \"actual_load\"   :actual_load[6001:],\n",
        "                \"actual_gen\"    :actual_gen[6001:],\n",
        "                \"bat_threshold\" :100,\n",
        "                \"bat_cap\"       :500,\n",
        "                \"purchase_price\":purchase_price[6001:],\n",
        "                \"num_preds\"     :6,\n",
        "                \"load_shedding\" :load_shedding[6001:],\n",
        "                \"wandb_log\"     : True,\n",
        "                \"train_log\"     : False\n",
        "}\n",
        "\n",
        "#define 5 environments   for training and eval\n",
        "\n",
        "eval_env = make_vec_env(EMS, n_envs = n_envs,env_kwargs = eval_args )\n",
        "\n",
        "\n",
        "#first run it with only standby (default)\n",
        "obs   = eval_env.reset()\n",
        "#ensure that the exit condition is reset\n",
        "done = [False]*n_envs\n",
        "#define the action to take\n",
        "action_standby = [0]*n_envs\n",
        "#reset score\n",
        "standby_score = [0]*n_envs\n",
        "standby_score = np.array(standby_score).astype(np.float32)\n",
        "while not all(done):\n",
        "    #step the model with the action\n",
        "    obs,reward,done,info = eval_env.step(action_standby)\n",
        "    #accumulate the score\n",
        "    standby_score += reward\n",
        "\n",
        "\n",
        "avg_standby_score = standby_score.mean()\n",
        "\n",
        "run.finish()\n",
        "\n",
        "eval_args = {\n",
        "                \"episode_len\"   :2760,\n",
        "                \"actual_load\"   :actual_load[6001:],\n",
        "                \"actual_gen\"    :actual_gen[6001:],\n",
        "                \"bat_threshold\" :100,\n",
        "                \"bat_cap\"       :500,\n",
        "                \"purchase_price\":purchase_price[6001:],\n",
        "                \"num_preds\"     :24,\n",
        "                \"load_shedding\" :load_shedding[6001:],\n",
        "                \"wandb_log\"     : True,\n",
        "                \"train_log\"     : False\n",
        "}\n",
        "\n",
        "#define 5 environments   for training and eval\n",
        "\n",
        "eval_env = make_vec_env(EMS, n_envs = n_envs,env_kwargs = eval_args )\n",
        "\n",
        "#Load model, fetch the latest (or whichever one you want from the model_dir)\n",
        "#Best A2C model:/content/drive/MyDrive/Colab Notebooks/EMSv1_1/models/A2C/EMSv1_1_A2C1010-081818.zip\n",
        "best_model =\"/content/drive/MyDrive/Colab Notebooks/EMSv2_1/models/PPO/EMSv2_1_PPO1017-110702.zip\"\n",
        "#best_model =\n",
        "#best_PPO_model\n",
        "#model_load = f\"{best_model}\"\n",
        "\n",
        "#model  = PPO.load(model_load, env = eval_env)\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"4022_intelligent_ems\",\n",
        "    config=config,\n",
        "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    save_code=True,  # optional\n",
        ")\n",
        "obs   = eval_env.reset()\n",
        "EMS_reward,EMS_std_reward = evaluate_policy(model,eval_env,n_eval_episodes = 1,deterministic=True)# callback = wandb_callback\n",
        "run.finish()\n",
        "\n",
        "print(f\"Note: The term does not refer to the cost in rands but rather to the reward as defined by the reward function!\")\n",
        "print(f\"Done the Standby Test! Total cost accumulated is: {avg_standby_score}\")\n",
        "print(f\"Done applying the trained model! Total cost accumulated is: {EMS_reward} +- {EMS_std_reward}\")\n",
        "\n",
        "savings = EMS_reward - avg_standby_score\n",
        "print(f\"The amount that was saved by applying the EMS agent: {savings}\")\n",
        "print(f\"This was saved over a period of {2760/24} days\")\n",
        "print(f\"The savings represents {(savings/(-avg_standby_score))*100} % of the cost if no EMS is installed\")\n",
        "print(f\"And it represents {(savings/(-EMS_reward))*100} % of the cost if the EMS is installed\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf4f04c7575f4e2190103b59884b65cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c7dd6f6898e405ab813073950533ef0",
              "IPY_MODEL_69e3459721d04ddebf2de2f3f6388336"
            ],
            "layout": "IPY_MODEL_a8305dffe04f4d25a0e00e4f69075486"
          }
        },
        "6c7dd6f6898e405ab813073950533ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac5c5ef5464547fba9902cff1b9bc057",
            "placeholder": "",
            "style": "IPY_MODEL_9ec3c3d85e944ac98a392e99350655b5",
            "value": "0.245 MB of 0.245 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "69e3459721d04ddebf2de2f3f6388336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02ce55d5e261434c80ea52c600e58abb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6d36a98851c48258d905d558cf57a2a",
            "value": 1
          }
        },
        "a8305dffe04f4d25a0e00e4f69075486": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5c5ef5464547fba9902cff1b9bc057": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ec3c3d85e944ac98a392e99350655b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02ce55d5e261434c80ea52c600e58abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6d36a98851c48258d905d558cf57a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}