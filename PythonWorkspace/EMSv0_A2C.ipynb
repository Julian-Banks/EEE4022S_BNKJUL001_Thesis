{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP69BAN/2UYRzWgo35U3TRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julian-Banks/EEE4022S_BNKJUL001_Thesis/blob/main/PythonWorkspace/EMSv0_A2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install stable_baselines3[extra]\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "MXzZxVUQFroQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Julian-Banks/EEE4022S_BNKJUL001_Thesis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu74obFw8Gts",
        "outputId": "d3630244-e9c2-41e6-a672-f6fc3abb9b3a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EEE4022S_BNKJUL001_Thesis'...\n",
            "remote: Enumerating objects: 411, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 411 (delta 31), reused 103 (delta 17), pack-reused 290\u001b[K\n",
            "Receiving objects: 100% (411/411), 141.49 MiB | 28.70 MiB/s, done.\n",
            "Resolving deltas: 100% (97/97), done.\n",
            "Updating files: 100% (289/289), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is very much a first working model and very simple\n",
        "First major problem is that it only has one load! no AC DC vibe\n",
        "also has no converter\n",
        "All of the loads and forecasts are random rn but I need to load in the real files.\n",
        "only one form of generation, no solar/wind difference\n",
        "\n",
        "Tech challenges:\n",
        "  If the data is normalised, how do I calculate the actual power used, in the batery exct, unless they are all normalised using the same metric,\n",
        "  could save thier normalisation values, un-normalise them to perform calculations and then renormalise them?\n",
        "  "
      ],
      "metadata": {
        "id": "DG4gB2cM7tDY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s2iW-k26FIbm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "070261f1-0402-4693-ea8e-f6cfdd8ce28d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "import datetime\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "class EMSv0(gym.Env):\n",
        "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self,bat_threshold = 0.1,purchase_price = [1,1,1,1,1,1,1,1,2,2,2,2] , episode_len = 8760,num_preds = 24,render_mode = \"none\"):\n",
        "\n",
        "        super(EMSv0, self).__init__()\n",
        "\n",
        "        #define time frame\n",
        "        self.current_step = 0\n",
        "        self.final_step = int(episode_len) #one years worth of steps\n",
        "\n",
        "        #Might make a function for these\n",
        "        #fill all of the actual loads NB!!! is just random for now NB!!! is normalised 0-1\n",
        "        self.actual_load = np.random.rand(self.final_step+num_preds+1).astype(np.float32) #will load from a file or something\n",
        "        #fill all of the actual generation steps.\n",
        "        self.actual_gen   = np.random.rand(self.final_step+num_preds+1).astype(np.float32) #will load from file or something\n",
        "\n",
        "        #define the purchase price for every step of the year\n",
        "        purchase_price = np.array(purchase_price).astype(np.float32)\n",
        "        repetitions    = (self.final_step+num_preds+1) // len(purchase_price)\n",
        "        remainder      = (self.final_step+num_preds+1) % len(purchase_price)\n",
        "        self.purchase_price =np.concatenate([purchase_price]*repetitions+[purchase_price[:remainder]])#need to read in from somewhere\n",
        "\n",
        "        #define var for storing the excess gen\n",
        "        self.excess_gen = 0\n",
        "        #define a var for determine amount purchased per step (dont want to make it total as this will incure growing penalties for the Agent if used in reward structure)\n",
        "        self.step_purchased = 0\n",
        "        #define the battery low threshold\n",
        "        self.bat_threshold = np.float32(bat_threshold)\n",
        "        #define default action\n",
        "        self.default_action = 0\n",
        "        #define actions and observations space\n",
        "        n_actions = 2 # keeping it simple\n",
        "\n",
        "        self.num_preds = num_preds # day ahead predictions\n",
        "        self.action_space = spaces.Discrete(n_actions)\n",
        "        # Dict space to store all the different things\n",
        "        self.observation_space = spaces.Dict({\n",
        "                \"load_forecast\": gym.spaces.Box(low=0, high=np.inf, shape=(1,num_preds), dtype=np.float32),\n",
        "                \"gen_forecast\": gym.spaces.Box(low=0, high=np.inf, shape=(1,num_preds), dtype=np.float32),\n",
        "                \"price_forecast\": gym.spaces.Box(low=0, high=np.inf, shape=(1,num_preds+1), dtype=np.float32),\n",
        "                \"bat_level\": gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
        "                \"current_load\": gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
        "                \"current_gen\": gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
        "                 })\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        #update the current state with the action (needs to be done before current_step is inc since we want to apply the action to the previous step to get the current state)\n",
        "        self.update_state(action)\n",
        "        #Calculate reward from the action\n",
        "        reward = self.calc_reward()\n",
        "\n",
        "        #inc time step into Future\n",
        "        self.current_step += 1\n",
        "        #get next observation (for next time step)\n",
        "        observation = self.get_obs()\n",
        "        #Set terminated to False since there are no failure states\n",
        "        self.terminated = False\n",
        "        #Check if timelimit reached\n",
        "        self.truncated = False if self.current_step<self.final_step else True\n",
        "        #dont know what to put into info for now\n",
        "        info = {}\n",
        "        return observation, reward, self.terminated, self.truncated, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed = seed, options=options)\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.terminated = False\n",
        "        self.truncated = False\n",
        "\n",
        "        #reset these cause I dont want the model to just memorise the random data (so it gets changed every reset)\n",
        "        #fill all of the actual loads NB!!! is just random for now NB!!! is normalised 0-1\n",
        "        self.actual_load = np.random.rand(self.final_step+self.num_preds+1).astype(np.float32) #will load from a file or something\n",
        "        #fill all of the actual generation steps.\n",
        "        self.actual_gen   = np.random.rand(self.final_step+self.num_preds+1).astype(np.float32) #will load from file or something\n",
        "\n",
        "\n",
        "        #reset the state\n",
        "        self.battery_level = 0.5\n",
        "        self.current_load = self.actual_load[0]\n",
        "        self.excess_gen = 0\n",
        "        self.step_purchased = 0\n",
        "        #get the first observation\n",
        "        observation = self.get_obs()\n",
        "        #Still don't know what to do with info\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def render(self):\n",
        "        #Reaaaaalllyyyy want to render something, Maybe the curent load as a point, the forecasts as a plot and the bat levels as bar\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        #don't think i need this for my application\n",
        "        pass\n",
        "\n",
        "    def update_state(self, action):\n",
        "        #Update current state with actions\n",
        "        if action == 0: #do nothing action\n",
        "            self.standby()\n",
        "        elif action == 1: #buy from Grid\n",
        "            self.purchase()\n",
        "        else:  #error case\n",
        "            raise ValueError(\n",
        "              f\"Received invalid action = {action} which is not part of the action space.\"\n",
        "            )\n",
        "        #case list for each action?\n",
        "\n",
        "    def calc_reward(self):\n",
        "        #Calculate reward based on the state\n",
        "        reward = -self.step_purchased*self.purchase_price[self.current_step]\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_obs(self):\n",
        "        #Fill the observation space with the next observation\n",
        "\n",
        "        #Get Forecasts Will probaly write a function for this? idk maybe a schlep to return all the info\n",
        "        load_forecast  = np.array( [self.actual_load[self.current_step+1: self.current_step + self.num_preds+1]] , dtype = np.float32) #will load from a file or something\n",
        "        if load_forecast.shape != (1,24):\n",
        "            print(f\"load_forecast shape is {load_forecast.shape}. Current step is {self.current_step}\")\n",
        "        gen_forecast   = np.array( [self.actual_gen[self.current_step+1: self.current_step + self.num_preds+1]] , dtype = np.float32) #will load from a file or something\n",
        "        if gen_forecast.shape != (1,24):\n",
        "            print(f\"gen_forecast shape is {gen_forecast.shape}. Current step is {self.current_step}\")\n",
        "        #get the prices for the current frame and the next 24 hours. Maybe will cut this down since that seems like a lot of info\n",
        "        price_forecast = np.array( [self.purchase_price[self.current_step:self.current_step+self.num_preds+1]] , dtype = np.float32)\n",
        "        #Just for readibility of the dict object\n",
        "        bat_level      = np.array([self.battery_level] , dtype= np.float32)\n",
        "        current_load   = np.array([self.actual_load[self.current_step]], dtype = np.float32)\n",
        "        current_gen    = np.array([self.actual_gen[self.current_step]], dtype  = np.float32)\n",
        "\n",
        "\n",
        "        obs = dict({\n",
        "                \"bat_level\":      bat_level,\n",
        "                \"current_gen\" :   current_gen,\n",
        "                \"current_load\":   current_load,\n",
        "                \"gen_forecast\":   gen_forecast,\n",
        "                \"load_forecast\":  load_forecast,\n",
        "                \"price_forecast\": price_forecast,\n",
        "        })\n",
        "        return obs\n",
        "\n",
        "    def standby(self):\n",
        "        #ems stands by, load is met by generation, battery and then grid\n",
        "        #if there is excess generation it is used to charge the batteries\n",
        "\n",
        "        #define step_gen and step_load for readability\n",
        "        step_gen  =  self.actual_gen[self.current_step]\n",
        "        step_load =  self.actual_load[self.current_step]\n",
        "        battery   =  self.battery_level\n",
        "        #check for gen meeting load\n",
        "        if step_load <= step_gen :\n",
        "            #set the purchased elect to 0 since gen meets load\n",
        "            self.step_purchased = 0\n",
        "            #calulate the excess elec that was generated\n",
        "            step_excess = step_gen - step_load\n",
        "            #check if battery needs to be charged\n",
        "            if battery < 1 :\n",
        "                #check if the excess amount that was generated is less than the available capacity\n",
        "                if 1-battery-step_excess > 0:\n",
        "                    self.battery_level += step_excess\n",
        "                else:\n",
        "                    #if the excess is greater than the availability then charge till full\n",
        "                    self.battery_level = 1\n",
        "                    #set step excess to excess minus the amount used to charge\n",
        "                    step_excess -= (1-battery)\n",
        "                    self.excess_gen += step_excess\n",
        "            else:\n",
        "                #if the battery is full then just inc excess_gen\n",
        "                self.excess_gen += step_excess\n",
        "        else:\n",
        "            #if the generation does not meet load\n",
        "            step_shortfall = step_load - step_gen\n",
        "            #checking if battery is above a threshold.\n",
        "            if battery > self.bat_threshold:\n",
        "                #check if battery has enough capacity to meet the load\n",
        "                if battery - step_shortfall >= self.bat_threshold:\n",
        "                    #if it does then subtract the shortfall from battery level\n",
        "                    self.battery_level -= step_shortfall\n",
        "                    #set the purchased variable to 0 since nothing was purchased\n",
        "                    self.step_purchased = 0\n",
        "                else:\n",
        "                    #set the battery to min value and purchase the rest from the grid\n",
        "                    self.battery_level = self.bat_threshold\n",
        "                    #calculate how much needs to be purchased\n",
        "                    step_shortfall -= (battery - self.bat_threshold)\n",
        "                    self.step_purchased = step_shortfall\n",
        "            else:\n",
        "                #no battery available, therefore everything needs to be bought from the grid.\n",
        "                self.step_purchased = step_shortfall\n",
        "\n",
        "    def purchase(self):\n",
        "        #purchase electricity to charge battery even if there is enough generation (I assume this will be used to buy at lower prices)\n",
        "        #get values for readability\n",
        "        step_load = self.actual_load[self.current_step]\n",
        "        step_gen  = self.actual_gen[self.current_step]\n",
        "        battery = self.battery_level\n",
        "\n",
        "        #calculate the total power need (the load plus the amount that the battery needs to charge)\n",
        "        total_need = step_load + (1-battery)\n",
        "        #if the generation is less than the need then purchase the remainder\n",
        "        if step_gen<total_need:\n",
        "            #purchashing the shortfall\n",
        "            self.step_purchased = total_need - step_gen\n",
        "            #setting the battery levels to full\n",
        "            self.battery_level = 1\n",
        "        else:\n",
        "            #if the gen is enough then set purchase to 0\n",
        "            self.step_purchased  = 0\n",
        "            #set the battery to fully charged\n",
        "            self.battery_level = 1\n",
        "            #inc excess_gen by caluclating the excess between the step gen and the total need (includes amount needed to charge the battery)\n",
        "            self.excess_gen += (step_gen - total_need)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the environment with stable_baselines3 check_env."
      ],
      "metadata": {
        "id": "2RUZKr7RrN-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "env = EMSv0()\n",
        "check_env(env,warn = True)"
      ],
      "metadata": {
        "id": "v4GSJ8LsidPq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "09e8cecf-8496-42bd-c4c8-8538860e2417"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation \n",
              "gen_forecast has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the \n",
              "observation to have only a 1D vector or use a custom policy to properly process the data.\n",
              "  warnings.warn(\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation \n",
              "gen_forecast has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the \n",
              "observation to have only a 1D vector or use a custom policy to properly process the data.\n",
              "  warnings.warn(\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation \n",
              "load_forecast has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the \n",
              "observation to have only a 1D vector or use a custom policy to properly process the data.\n",
              "  warnings.warn(\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation \n",
              "load_forecast has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the \n",
              "observation to have only a 1D vector or use a custom policy to properly process the data.\n",
              "  warnings.warn(\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation \n",
              "price_forecast has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the \n",
              "observation to have only a 1D vector or use a custom policy to properly process the data.\n",
              "  warnings.warn(\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation \n",
              "price_forecast has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the \n",
              "observation to have only a 1D vector or use a custom policy to properly process the data.\n",
              "  warnings.warn(\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the base model (no EMS, just using standby mode)"
      ],
      "metadata": {
        "id": "6Kvh0dymrVgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the base environment\n",
        "base_env = EMSv0(episode_len = 7*24)\n",
        "#reset the environment and save the obs\n",
        "#going to run it 100 times to get a benchmark\n",
        "#reset score\n",
        "score = 0\n",
        "for step in range(1000):\n",
        "    obs,_    = base_env.reset()\n",
        "    #ensure that the exit condition is reset\n",
        "    truncated = False\n",
        "    #define the action to take\n",
        "    action_standby = 0\n",
        "\n",
        "    while not truncated:\n",
        "        obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "        score += reward\n",
        "\n",
        "print(f\"Done iteration! Total reward accumulated is: {score/step}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1vMEdXW4otUB",
        "outputId": "1103d05b-b5ae-49e2-a83e-04f41e09cbaa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Done iteration! Total reward accumulated is: -14.025860077784014\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Done iteration! Total reward accumulated is: -14.025860077784014\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a model"
      ],
      "metadata": {
        "id": "qRy0KQKOriqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a new environment to train the model in.\n",
        "train_env = EMSv0(episode_len = 7*24)\n",
        "\n",
        "#mount the drive\n",
        "drive.mount('/content/drive')\n",
        "#define paths to logs and model saves\n",
        "model_dir = \"/content/drive/MyDrive/Colab Notebooks/EMSv0/models/A2C/\"\n",
        "log_dir   = \"/content/drive/MyDrive/Colab Notebooks/EMSv0/models/A2Clogs/\"\n",
        "\n",
        "#make the appropriate directory if it does not exist\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "kqy-glMavKA6",
        "outputId": "09f1e4de-2bae-4751-85df-6ff27c2cae10"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", \n",
              "force_remount=True).\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", \n",
              "force_remount=True).\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD OR MAKE MODEL HERE!**"
      ],
      "metadata": {
        "id": "7pAj6-n04zyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Create the model with the MultiInputPolicy, use the training env, verbose is off because tensorboard loging is enabled\n",
        "#model = A2C(\"MultiInputPolicy\",train_env, verbose = 0, tensorboard_log = log_dir)\n",
        "\n",
        "#Load model, fetch the latest (or whichever one you want from the model_dir)\n",
        "#model_load = \"/content/drive/MyDrive/Colab Notebooks/EMSv0/models/A2C/EMSv0_A2C20231004-0826.zip\"\n",
        "#model  = A2C.load(model_load, env = train_env)\n",
        "\n"
      ],
      "metadata": {
        "id": "gQDhUNGWeum4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/Colab Notebooks/EMSv0/models/A2Clogs/\"\n",
        "\n",
        "while True:\n",
        "    #define the name for the specific log\n",
        "    log_name = \"EMSv0_A2C\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    #make the model learn, set the reset to false so that it keeps its old learning\n",
        "    model.learn(total_timesteps= 100000, tb_log_name = log_name,reset_num_timesteps=False)\n",
        "    model.save(f\"{model_dir}{log_name}\")\n",
        "    #open tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "g5dU9TLFzhxD",
        "outputId": "b0d4c244-0a38-4c0c-f957-49be4dbb26f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Launching TensorBoard..."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define a test environment\n",
        "test_env = EMSv0\n",
        "#reset the environment and save the obs\n",
        "obs,_    = base_env.reset()\n",
        "#ensure that the exit condition is reset\n",
        "truncated = False\n",
        "#define the action to take\n",
        "action_standby = 0\n",
        "#reset score\n",
        "score = 0\n",
        "while not truncated:\n",
        "    #predict the best next action\n",
        "    action,_ = model.predict(obs)\n",
        "    #step the model with the action\n",
        "    obs,reward,terminated,truncated,info = base_env.step(action)\n",
        "    #accumulate the score\n",
        "    score += reward\n",
        "print(f\"Done! Total reward accumulated is: {score}\")"
      ],
      "metadata": {
        "id": "inDHszZaxBUS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}