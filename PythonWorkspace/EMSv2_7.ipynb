{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julian-Banks/EEE4022S_BNKJUL001_Thesis/blob/main/PythonWorkspace/EMSv2_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V6yT1Kii6fZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Version Notes**\n",
        "\n",
        "# **v2.7**\n",
        " **options**\n",
        "\n",
        " * include real forecasts\n",
        " * include demand charge\n",
        " * try either of these options with or without loadshedding?\n",
        "\n",
        "#**v2.6**\n",
        "*clean up of code\n",
        "\n",
        "#**v2.5**\n",
        "* getting rid of the shorter episodes and going back to the normal approach\n",
        "* reworked normalisation so that the power bal and the battery level are scaled using the same transform. The idea being that the units should be the same and thus the scales?\n",
        "* changed action space to be centered on -1,1\n",
        "*removed demand charge to try and get it to learn how to manage a continuous space first.\n",
        "\n",
        "\n",
        "# **v2.4**\n",
        "* going to have shorter episodes (one month long)\n",
        "* hope the agent can learn to expect the demand charge every 30 days.\n",
        "* fixed a bug with the purchase_price\n",
        "\n",
        "# **v2.3**\n",
        "* Flattening observation space\n",
        "* increasing learning rates\n",
        "* decreasing vf_coef\n",
        "\n",
        "# **v2.2**\n",
        " **Added:**\n",
        " * normalising values. it is time to do this.....\n",
        "\n",
        "# **v2.1**\n",
        "\n",
        "**Added:**\n",
        " * changed reward structure to give a penalty every time the agent reaches a new max demand.\n",
        " * changed obs to include max_demand\n",
        "\n",
        "# **v2.0**\n",
        "*\n",
        "Adding a continous action space! Yolo\n",
        "\n",
        "# **v1.2**\n",
        "**Added:**\n",
        "* Diesel Generator action to mitigate unmet-load\n",
        "* reward based off real prices + demand charge - Demand charge has fucked the agent cause it buys in bulk! - might be time for the continous action space so it can decide how much to buy......\n",
        "\n",
        "**To Do:**\n",
        "* impliment a priority load - not gonna do this, just gonna have unmet load\n",
        "\n",
        "* Add in actual predictions, eish\n",
        "\n",
        "* figure out how to have more episodes!! I think it will have big performance boosts on the training :)\n",
        "\n",
        "* thinking that maybe I should change obs space back to what it was cause I dont actully need to know individual loads and gens!!!!\n",
        "\n",
        "# **v1.1**\n",
        "\n",
        "**Added:**\n",
        "* rect and inverter power tracking\n",
        "* reward logging in my own logging func\n",
        "* changed logging vars to arrays\n",
        "*\n",
        "\n",
        "**To Do**\n",
        "\n",
        "* battery charging rates - I think my assumption is fine.\n",
        "\n",
        "* tweak visualisation to show bar graphs at the end of training/testing. Maybe just print graphs at the end? I have added plt.show() - remember to play if it doesnt work!\n",
        "\n",
        "* impliment a generator!!!!!\n",
        "* impliment a priority load\n",
        "\n",
        "* NB figure out how to have more episodes!! I think it will have big performance boosts on the training :)\n",
        "\n",
        "* thinking that maybe I should change obs space back to what it was cause I dont actully need to know individual loads and gens!!!!\n",
        "\n",
        "# **v1.0**\n",
        "\n",
        "**Added:**\n",
        "* AC and DC load\n",
        "* Wind Gen\n",
        "* changed obs space to hold new loads\n",
        "* re wrote standby and purchase functions\n",
        "\n",
        "**To DO**\n",
        "* thinking that maybe I should change obs space back to what it was cause I dont actully need to know individual loads and gens!!!!\n",
        "* Add in rectifier & inverter power tracking\n",
        "* battery charging rates\n",
        "\n",
        "\n",
        "# **v0.3**\n",
        "**Added:**\n",
        "* loadshedding\n",
        "* New reward structure\n",
        "* added Vec_env\n",
        "* added eval callbacks to validate training\n",
        "* added in logging\n",
        "\n",
        "**Parameters:**\n",
        "* Added in loadshedding forecast\n",
        "\n",
        "**To do:**\n",
        "* find out about battery charging rates  & impliment\n",
        "* Find out if I need to normalise\n",
        "* Try dqn\n",
        "* Find out how the bounds for the obs_space box effect things\n",
        "* Try play with DummyVec wrapper (didnt work in last version)\n",
        "\n",
        "# **v0.2_1**\n",
        "**Added:**\n",
        "*  Monitor wrapper\n",
        "*  DummyVec wrapper\n",
        "*  Wand (weights and bais) enabled\n",
        "\n",
        "**Parameters:**\n",
        "* lowered to 3 predictions  \n",
        "\n",
        "**To do:**\n",
        "* Try to use hyperparameter Optimisation - decided Im only going to do on the final version\n",
        "* Try normalise\n",
        "* Try differnet models\n",
        "* Find out how the bounds for the obs_space box effect things\n",
        "\n",
        "# **v0.2**\n",
        "**Added:**\n",
        "*  simplified load_forecast and gen_forecast to be power_bal_forecasts.\n",
        "*  combined current_load and current_gen to also show current_power_balance\n",
        "*  added proper evaluate call\n",
        "\n",
        "**Parameters:**\n",
        "* No changes  \n",
        "\n",
        "**Results:**\n",
        "* 5% savings on PPO deterministic = true\n",
        "* 4.3% savings on PPO determnistic = false\n",
        "\n",
        "**To do:**\n",
        "* Try to use hyperparameter Optimisation\n",
        "* Try normalise\n",
        "* Try differnet models\n",
        "* Try see if discount rate can be tweaked - at good level.\n",
        "* Find out how the bounds for the obs_space box effect things\n",
        "\n",
        "# **v0.1**\n",
        "**Added:**\n",
        "* added real loads, gen, tou_id's\n",
        "\n",
        "**Parameters:**\n",
        "* training episode = 6000 timesteps\n",
        "* testing episode  = 2760 timesteps\n",
        "* bat_threshold = 100\n",
        "* bat_cap = 500\n",
        "* battery_level at reset = bat_cap/2\n",
        "* num_preds = 24\n",
        "* Trained PPO for 1.65mil timesteps\n",
        "* Trained A2C for 1.2 mil timesteps\n",
        "\n",
        "**Results:**\n",
        "* PP0 - 3.7% improvement from standby mode Deterministic = False\n",
        "* PPO - 6.1% Deterministic =  True\n",
        "* A2C  - -0.3% improvement. And the models after this got worse as training progressed!\n",
        "**To do:**\n",
        "* Try lower num_preds\n",
        "* Try to use hyperparameter Optimisation\n",
        "* Try normalise\n",
        "* Try differnet models\n",
        "* Try see if discount rate can be tweaked\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5DZFnZnzStt1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nI52iVVCCPaf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#install dependancies\n",
        "!pip install gymnasium\n",
        "!pip install stable_baselines3[extra]\n",
        "!pip install wandb\n",
        "!pip install sklearn\n",
        "!pip install scipy\n",
        "%load_ext tensorboard\n",
        "\n",
        "#clone repository\n",
        "! git clone https://github.com/Julian-Banks/EEE4022S_BNKJUL001_Thesis\n",
        "\n",
        "#to update the rep\n",
        "%cd /content/EEE4022S_BNKJUL001_Thesis\n",
        "! git pull\n",
        "#import needed libarys\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from gymnasium import spaces\n",
        "import datetime\n",
        "from stable_baselines3 import PPO, A2C, DQN,DDPG\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from gym.wrappers import FlattenObservation\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "import os\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from gymnasium.envs.registration import register\n",
        "import scipy.io\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#mount the drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define paths to logs and model saves\n",
        "model_type = \"PPO\"\n",
        "version    = \"EMSv2_7\"\n",
        "model_dir = f\"/content/drive/MyDrive/Colab Notebooks/{version}/models/{model_type}/\"\n",
        "log_dir   = f\"/content/drive/MyDrive/Colab Notebooks/{version}/models/{model_type}logs/\"\n",
        "\n",
        "#make the appropriate directory if it does not exist\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n"
      ],
      "metadata": {
        "id": "SIpnNCVVRTV8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define our environment class!!**"
      ],
      "metadata": {
        "id": "Q99_jLMvwuZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "s2iW-k26FIbm"
      },
      "outputs": [],
      "source": [
        "class EMSv2_7(gym.Env):\n",
        "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\",\"rgb_array\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self,bat_threshold = 0.1, bat_cap = 1, actual_load = None, actual_gen = None, load_forecast = None , gen_forecast = None, purchase_price = [1,1,1,1,1,1,1,1,2,2,2,2] , episode_len = 8760,num_preds = 24,render_mode = \"rgb_array\", load_shedding = \"none\", wandb_log = False,train_log = True, gen_size = 100,demand_charge = 252.92):\n",
        "\n",
        "        super(EMSv2_7, self).__init__()\n",
        "        #define render_mode\n",
        "        self.render_mode = render_mode\n",
        "        #define wandb\n",
        "        self.wandb_log = wandb_log\n",
        "        self.train_log = train_log\n",
        "        #define time frame\n",
        "        self.current_step = 0\n",
        "        self.final_step = int(episode_len)-num_preds-2\n",
        "        self.episode_len = int(episode_len)\n",
        "        #define num preds\n",
        "        self.num_preds = int(num_preds)\n",
        "        #define the battery max capacity\n",
        "        self.bat_cap =  np.float32(bat_cap)\n",
        "        #define the battery low threshold\n",
        "        self.bat_threshold = np.float32(bat_threshold)\n",
        "        #define demand charge\n",
        "        self.demand_charge = demand_charge\n",
        "        #define the size of the diesel_gen\n",
        "        self.gen_size = gen_size\n",
        "\n",
        "        #fill all of the actual loads\n",
        "        self.fill_load(actual_load)\n",
        "\n",
        "        #fill all of the actual generation steps.\n",
        "        self.fill_gen(actual_gen)\n",
        "\n",
        "        #fill the total power balance\n",
        "        self.fill_power_bal()\n",
        "\n",
        "        #Fill the loadShedding indicator\n",
        "        self.fill_shedding(load_shedding)\n",
        "\n",
        "        #define the purchase price for every step of the year\n",
        "        self.fill_price(purchase_price)\n",
        "\n",
        "        #Fill the load forecasts\n",
        "        self.fill_load_forecast(load_forecast)\n",
        "\n",
        "        #fill the gen Forecast\n",
        "        self.fill_gen_forecast(gen_forecast)\n",
        "\n",
        "        #action space is recomended to be -1 to 1. by stablebaslines due to the sampling distributions\n",
        "        self.action_scaler = MinMaxScaler(feature_range = (-1,1))\n",
        "        #set the lower bound of the action to 0, (requesting to buy 0 kw) and the upper to the max_avail capacity and the max load required.\n",
        "        self.action_scaler.fit_transform(np.array([0, self.bat_cap -  self.bat_threshold + np.max(self.actual_load)]).reshape(-1,1))\n",
        "        #define the size of the action space\n",
        "        self.action_space = gym.spaces.Box(low= -1 , high =1)\n",
        "\n",
        "        # Dict space to store all the different things\n",
        "        self.observation_space = spaces.Dict({\n",
        "                \"power_bal_forecast\" : gym.spaces.Box(low=-1, high=1, shape=(1,num_preds), dtype=np.float32),\n",
        "                \"price_forecast\"     : gym.spaces.Box(low=0, high=1, shape=(1,num_preds+1), dtype=np.float32),\n",
        "                \"island_forecast\"    : gym.spaces.Box(low=0, high=1, shape=(1,num_preds+1), dtype=np.float32),\n",
        "                \"bat_level\"          : gym.spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
        "                \"current_power_bal\"  : gym.spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
        "                })\n",
        "        #flatten the observation space to a 1D vector so that it can be interperted more easily by the agent.\n",
        "        self.observation_space = spaces.flatten_space(self.observation_space)\n",
        "\n",
        "    def step(self, action):\n",
        "        #update the current state with the action (needs to be done before current_step is inc since we want to apply the action to the previous step to get the current state)\n",
        "        self.update_state(action)\n",
        "\n",
        "        #Calculate reward from the action\n",
        "        self.reward[self.current_step] = self.calc_reward()\n",
        "        reward = self.reward[self.current_step]\n",
        "\n",
        "        #Wand log, if its set to true(so that it only gets run when wandb is initialised)\n",
        "        # if train_log is false then every step is logged by the eval logger. (only the final step is logged by the train logger)\n",
        "        if self.train_log != True and self.wandb_log ==True:\n",
        "            self.wandb_logger()\n",
        "\n",
        "        #inc time step into Future\n",
        "        self.current_step += 1\n",
        "        #get the next observation for the agent (for next time step)\n",
        "        observation = self.get_obs()\n",
        "\n",
        "        #Set terminated to False since there are no failure states\n",
        "        self.terminated = False\n",
        "        #Check if timelimit reached\n",
        "        self.truncated = False if self.current_step<self.final_step else True\n",
        "        #if the episode is truncated, log the final step(both Training and Eval modes)\n",
        "        if self.truncated and self.wandb_log:\n",
        "            self.wandb_logger()\n",
        "\n",
        "        #requirement but I am not using it so it is empty\n",
        "        info = {}\n",
        "        return observation, reward, self.terminated, self.truncated, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed = seed, options=options)\n",
        "        #reset the state\n",
        "        self.current_step = 0\n",
        "        self.terminated = False\n",
        "        self.truncated = False\n",
        "\n",
        "        #Reset values to run Grid\n",
        "        self.battery_level      = np.zeros(self.final_step+1)\n",
        "        self.battery_level[0]   = self.power_scaler.transform(np.array(self.bat_cap/2).reshape(-1,1))#set the intial battery level to half its capacity\n",
        "        self.reward             = np.zeros(self.final_step+1)\n",
        "        self.step_purchased     = np.zeros(self.final_step+1)\n",
        "\n",
        "        #Reset all of the other variables used to log\n",
        "        self.excess_gen         = np.zeros(self.final_step+1)\n",
        "        self.step_unmet_load    = np.zeros(self.final_step+1)\n",
        "        self.off_peak_purchases = np.zeros(self.final_step+1)\n",
        "        self.peak_purchases     = np.zeros(self.final_step+1)\n",
        "        self.standard_purchases = np.zeros(self.final_step+1)\n",
        "        self.off_peak_cost      = np.zeros(self.final_step+1)\n",
        "        self.standard_cost      = np.zeros(self.final_step+1)\n",
        "        self.peak_cost          = np.zeros(self.final_step+1)\n",
        "        self.step_invt          = np.zeros(self.final_step+1)\n",
        "        self.step_rect          = np.zeros(self.final_step+1)\n",
        "        self.diesel_gen         = np.zeros(self.final_step+1)\n",
        "        self.action_purchase    = np.zeros(self.final_step+1)\n",
        "        self.money_spent        = np.zeros(self.final_step+1)\n",
        "\n",
        "        #get the first observation\n",
        "        observation = self.get_obs()\n",
        "        #Still don't know what to do with info\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def render(self, mode='rgb_array', save_path=None):\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        #don't think i need this for my application\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "    #MAIN HELPER FUNCTIONS\n",
        "    #######################################################################################################################################################################\n",
        "\n",
        "    def update_state(self, action):\n",
        "        #impliment the action into the environment\n",
        "        self.run_grid(purchase_amount = action)\n",
        "        #log the agents action (in kw)\n",
        "        self.action_purchase[self.current_step] = self.action_scaler.inverse_transform(np.array([action]).reshape(-1,1))\n",
        "        #calculate and log the purchases per Time of Use rate\n",
        "        self.tou_purchase_inc()\n",
        "        #Calculate the flow of power\n",
        "        #fetch info from grids\n",
        "        self.calc_power_flow()\n",
        "\n",
        "    def run_grid(self,purchase_amount):\n",
        "        #self.power_scaler.inverse_transform(self.actual_power_bal[self.current_step].reshap(-1,1))\n",
        "        #fetch info from grids\n",
        "        ac_power_bal, avail_grid = self.AC_bus()\n",
        "        dc_power_bal, avail_bat, avail_stor = self.DC_bus()\n",
        "        #fecth the current_battery level for this step.\n",
        "        current_battery_level =  self.power_scaler.inverse_transform(self.battery_level[self.current_step].reshape(-1,1))\n",
        "        #un-normalise the agents purchase request (its action)\n",
        "        purchase_amount_unnorm = self.action_scaler.inverse_transform(np.array([purchase_amount]).reshape(-1,1))\n",
        "\n",
        "        if avail_grid: # if there is no loadshedding purchase the amount requested by the agent.\n",
        "            self.step_purchased[self.current_step] = purchase_amount_unnorm\n",
        "        else:          # if there is loadshedding then it the step purchased must be 0\n",
        "            self.step_purchased[self.current_step] = 0\n",
        "\n",
        "        #calculate the immediate power_bal, this is the AC power, DC power and the Amount that the agent has purchased\n",
        "        grid_power_bal = ac_power_bal + dc_power_bal + self.step_purchased[self.current_step]\n",
        "\n",
        "        #determine the flow of power:\n",
        "        if grid_power_bal > 0 : #if the power balance is positive, the battery is charged and any remaing power is recorded in excess gen\n",
        "            #Set the battery_level for the next step to the current battery plus the exces. Using minimium between avail_stor and grid_power_bal ensures that the battery is only charged to its max capacity.\n",
        "            self.battery_level[self.current_step+1] =  self.power_scaler.transform(current_battery_level+ min(avail_stor, grid_power_bal))\n",
        "            #increments excess gen by the max. If grid_power_bal - avail_stor is negative, there was no excess and it will add 0, else it will add the excess that couln't be stored.\n",
        "            self.excess_gen[self.current_step] = max((grid_power_bal - avail_stor), 0)\n",
        "        else:\n",
        "            #there is a shortage of power since the balance is negative, see if we can take it from the battery.\n",
        "            #set the battery level for the next step. If the battery has the avail capacity to meet the demand, then the demand is subtracted. If the battery does not then the avail_battery is subtracted. This prevents over discharge.\n",
        "            self.battery_level[self.current_step+1] = self.power_scaler.transform(current_battery_level + max(-avail_bat, grid_power_bal))\n",
        "\n",
        "            #check if there is still a shortage of power after the battery has been discharged.\n",
        "            grid_power_bal_discharged = grid_power_bal + min(avail_bat,-grid_power_bal)\n",
        "            #if there is still a shortage, Check if it can be purchased from the grid or if it will be added to unmet_load\n",
        "            if grid_power_bal_discharged  < 0 :\n",
        "            #check if we are islanded and buy elec if we arent\n",
        "                if avail_grid:\n",
        "                    #set the purchased amount for the step to be the amount purchased already, and the remaining amount needed to balance the power.\n",
        "                    self.step_purchased[self.current_step] = purchase_amount_unnorm + -grid_power_bal_discharged\n",
        "                else:\n",
        "                    #if the grid is not available, run the generator\n",
        "                    #the amount of generation provided by the generator is limited by its size\n",
        "                    self.diesel_gen[self.current_step] = min(-grid_power_bal_discharged,self.gen_size)\n",
        "                    #calulate grid_power_bal after the diesel generator has been used\n",
        "                    grid_power_bal_with_diesel = grid_power_bal_discharged + min(-grid_power_bal_discharged,self.gen_size)\n",
        "                    if grid_power_bal_with_diesel < 0:\n",
        "                        self.step_unmet_load[self.current_step] = -grid_power_bal_with_diesel\n",
        "\n",
        "    def calc_reward(self):\n",
        "        #Calculate reward based on the state\n",
        "        #cost of running generator in that step\n",
        "        petrol_per_kw  = 0.4*23 #0.4l per kwh produced multipled by a cost of 23 rand per litre. Very rough values\n",
        "        diesel_cost     = petrol_per_kw*self.diesel_gen[self.current_step]\n",
        "        #cost of purchasing electricity in that step\n",
        "        elec_purchase   = self.step_purchased[self.current_step]*self.price_scaler.inverse_transform(self.purchase_price[self.current_step].reshape(-1,1)).flatten()\n",
        "        #Penality for unmet_load\n",
        "        unmet_load_pen  = self.step_unmet_load[self.current_step]*10\n",
        "\n",
        "        demand_penalty = self.get_demand_penalty()\n",
        "\n",
        "\n",
        "        demand_charge = self.get_demand_charge()\n",
        "\n",
        "        #Calculate the reward (monetary cost and the added penality for unmet_load!)\n",
        "        reward = -elec_purchase - unmet_load_pen - diesel_cost - demand_penalty\n",
        "        #record the money spent in this step\n",
        "        self.money_spent[self.current_step] =  elec_purchase +  diesel_cost + demand_charge\n",
        "        return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_obs(self):\n",
        "        #Fill the observation space with the next observation\n",
        "\n",
        "        #calculate the power forecast\n",
        "        power_bal_forecast = self.get_forecast() #self.actual_power_bal[self.current_step+1: self.current_step + self.num_preds+1]\n",
        "\n",
        "        #get the prices for the current frame and the next 24 hours. Maybe will cut this down since that seems like a lot of info\n",
        "        price_forecast = np.array( [self.purchase_price[self.current_step:self.current_step+self.num_preds+1]] , dtype = np.float32).reshape(1,self.num_preds+1)\n",
        "        #Just for readibility of the dict object\n",
        "        bat_level      = np.array([self.battery_level[self.current_step]] , dtype= np.float32)\n",
        "\n",
        "        #island forecasst, same as tou forecast\n",
        "        island_forecast =np.array( [self.load_shed[self.current_step:self.current_step+self.num_preds+1]] , dtype = np.float32)\n",
        "\n",
        "        #calculate the current power balance\n",
        "        current_power_bal = self.actual_power_bal[self.current_step]\n",
        "\n",
        "        obs = dict({\n",
        "                \"bat_level\":      bat_level,\n",
        "                \"current_power_bal\" :   current_power_bal,\n",
        "                \"island_forecast\": island_forecast,\n",
        "                \"power_bal_forecast\":  power_bal_forecast,\n",
        "                \"price_forecast\": price_forecast,\n",
        "\n",
        "        })\n",
        "        #flatten the obs to make it the correct shape. 1D array\n",
        "        obs = np.concatenate([obs[key].flatten() for key in obs.keys()])\n",
        "        return obs\n",
        "\n",
        "    def wandb_logger(self):\n",
        "\n",
        "        train_log_dict={\n",
        "                    \"Excess Generation\"         :np.sum(self.excess_gen),\n",
        "                    \"Unmet Load\"                :np.sum(self.step_unmet_load),\n",
        "                    \"Off-Peak Purchases\"        :np.sum(self.off_peak_purchases),\n",
        "                    \"Standard Purchases\"        :np.sum(self.standard_purchases),\n",
        "                    \"Peak Purchases\"            :np.sum(self.peak_purchases),\n",
        "                    \"Num Off-Peak Purchases\"    :np.count_nonzero(self.off_peak_purchases),\n",
        "                    \"Num Standard Purchases\"    :np.count_nonzero(self.standard_purchases),\n",
        "                    \"Num Peak Purchases\"        :np.count_nonzero(self.peak_purchases),\n",
        "                    \"Total Elec Purchase\"       :np.sum(self.step_purchased),\n",
        "                    \"Total Purchase Requested\"  :np.sum(self.action_purchase),\n",
        "                    \"Num Diesel Gen Actions\"    :np.count_nonzero(self.diesel_gen),\n",
        "                    \"Total Reward\"              :np.sum(self.reward),\n",
        "                    \"Total Money spent\"         :np.sum(self.money_spent),\n",
        "                    \"Rectifier total power flow\":np.sum(self.step_rect),\n",
        "                    \"Inverter total power flow\" :np.sum(self.step_invt),\n",
        "                    \"Diesel Generator\"          :np.sum(self.diesel_gen),\n",
        "                    \"Max Demand\"                :np.max(self.step_purchased),\n",
        "                    \"total Purchased Elec\"      :np.sum(self.step_purchased)\n",
        "                    }\n",
        "\n",
        "        eval_log_dict={\n",
        "                    \"battery_level\"             :self.power_scaler.inverse_transform(self.battery_level[self.current_step].reshape(-1,1)).flatten(),\n",
        "                    \"AC load\"                   :self.actual_load[self.current_step,0],\n",
        "                    \"DC load\"                   :self.actual_load[self.current_step,1],\n",
        "                    \"Wind generation\"           :self.actual_gen[self.current_step,0],\n",
        "                    \"PV generation\"             :self.actual_gen[self.current_step,1],\n",
        "                    \"Excess Generation\"         :self.excess_gen[self.current_step],\n",
        "                    \"Unmet Load\"                :self.step_unmet_load[self.current_step],\n",
        "                    \"LoadShedding\"              :self.load_shed[self.current_step],\n",
        "                    \"Off-Peak Purchases\"        :self.off_peak_purchases[self.current_step],\n",
        "                    \"Standard Purchases\"        :self.standard_purchases[self.current_step],\n",
        "                    \"Peak Purchases\"            :self.peak_purchases[self.current_step],\n",
        "                    \"Num Off-Peak Purchases\"    :np.count_nonzero(self.off_peak_purchases),\n",
        "                    \"Num Standard Purchases\"    :np.count_nonzero(self.standard_purchases),\n",
        "                    \"Num Peak Purchases\"        :np.count_nonzero(self.peak_purchases),\n",
        "                    \"Step Purchased\"            :self.step_purchased[self.current_step],\n",
        "                    \"Purchase Requested\"        :self.action_purchase[self.current_step],\n",
        "                    \"Num Diesel Gen Actions\"    :np.count_nonzero(self.diesel_gen),\n",
        "                    \"Total Reward\"              :self.reward[self.current_step],\n",
        "                    \"Money Spent\"               :self.money_spent[self.current_step],\n",
        "                    \"Rectifier total power flow\":self.step_rect[self.current_step],\n",
        "                    \"Inverter total power flow\" :self.step_invt[self.current_step],\n",
        "                    \"Diesel Generator\"          :self.diesel_gen[self.current_step],\n",
        "\n",
        "                    }\n",
        "\n",
        "        if self.train_log:\n",
        "            wandb.log(train_log_dict)\n",
        "        else:\n",
        "            wandb.log(eval_log_dict)\n",
        "\n",
        "        if self.train_log == False and self.current_step == self.final_step:\n",
        "\n",
        "            values = [[np.sum(self.off_peak_purchases),'Off Peak'], [np.sum(self.standard_purchases),'Standard'], [np.sum(self.peak_purchases),'Peak']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            Tou_purchases = wandb.plot.bar(table,\"labels\",\"values\", title=\"kWh per TOU tariff\")\n",
        "\n",
        "            values = [[np.sum(self.off_peak_cost),'Off Peak'], [np.sum(self.standard_cost),'Standard'], [np.sum(self.peak_cost),'Peak']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            Tou_purchase_cost = wandb.plot.bar(table,\"labels\",\"values\", title=\"Rands per TOU tariff\")\n",
        "\n",
        "            values = [[np.max(self.step_purchased),'Max demand'], [np.max(self.diesel_gen),'Max diesel gen'], [np.max(self.step_rect),'Max rectifier power'], [np.max(self.step_invt),'Max inverter power']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            max_metrics = wandb.plot.bar(table,\"labels\",\"values\", title=\"Sizing Metrics\")\n",
        "\n",
        "            values = [[np.sum(self.step_purchased),'Total Purchased'], [np.sum(self.action_purchase),'Total requested purchases'],[np.sum(self.excess_gen),'Total excess generation']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            total_metrics = wandb.plot.bar(table,\"labels\",\"values\", title=\"Total Metrics\")\n",
        "\n",
        "            values = [[np.sum(self.diesel_gen),'Total diesel generation'], [np.sum(self.step_unmet_load),'Total unmet load']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            total_diesel_unmet = wandb.plot.bar(table,\"labels\",\"values\", title=\"Total diesel and unmet load\")\n",
        "\n",
        "            values = [[np.sum(self.reward),'Total reward accumulated']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            total_reward = wandb.plot.bar(table,\"labels\",\"values\", title=\"Total reward\")\n",
        "\n",
        "            values = [[np.sum(self.money_spent),'Total money spent']]\n",
        "            table = wandb.Table(columns = [\"values\",\"labels\"],data=values)\n",
        "            total_spent = wandb.plot.bar(table,\"labels\",\"values\", title=\"Total money spent\")\n",
        "\n",
        "            wandb.log({ \"kWh purchased per TOU tariff\"  : Tou_purchases,\n",
        "                        \"Rands per TOU tariff\"          : Tou_purchase_cost,\n",
        "                        \"Max sizing metrics\"            : max_metrics,\n",
        "                        \"Total metrics\"                 : total_metrics,\n",
        "                        \"Total diesel and unmet load\"   : total_diesel_unmet,\n",
        "                        \"Total reward and money spent\"  :total_reward,\n",
        "                        \"Total money spent\"             :total_spent\n",
        "                        })\n",
        "\n",
        "\n",
        "\n",
        "    #AUXILIARY HELPER FUNCTIONS\n",
        "    #######################################################################################################################################################################\n",
        "    def calc_power_flow(self):\n",
        "        #fetch info from grids\n",
        "        ac_power_bal, _ = self.AC_bus()\n",
        "        dc_power_bal, _ , avail_stor = self.DC_bus()\n",
        "        #Calculate the flow of power (this is max absorb, the min need is just dc_power_bal)\n",
        "        dc_power_absorb = max(-dc_power_bal+avail_stor,0)\n",
        "        #calculate the Dc_power_avail, could go to battery or AC_grid\n",
        "        dc_power_avail = max(dc_power_bal,0) #lol just defined a new var for readability.\n",
        "        #ac power excess will be the power balance added to the purchase amount\n",
        "        ac_power_excess = max(ac_power_bal+self.step_purchased[self.current_step]+self.diesel_gen[self.current_step],0)\n",
        "         # calculate how much power the ac grid needs.\n",
        "        ac_power_need   = max(-ac_power_bal-self.step_purchased[self.current_step]-self.diesel_gen[self.current_step], 0)\n",
        "        #calculate how much power would be in excess if there was to be excess.\n",
        "        dc_power_excess = max(dc_power_avail - ac_power_need - avail_stor,0)\n",
        "\n",
        "        #the power that will flow through the rectifier is the minimum between the amount the DC grid can absorb and the excess the ac_grid has\n",
        "        #This definition implies that the AC subgrid will supply its own loads first and only then send excess to dc Grid?\n",
        "        rect_power = min(dc_power_absorb,ac_power_excess)\n",
        "\n",
        "        #inverter power is equal to the minimum val between the avail dc power and the needed power. Then since all excess power needs to go to the grid, if there is any extra energy after the ac_need has been met and the battery is fully charged, it is also sent through the inverter\n",
        "        #DC subgrid will meet the AC load before charging the battery. Will send excess across if the DC load is met and the battery is fully charged.\n",
        "        invt_power = min(ac_power_need, dc_power_avail) + dc_power_excess\n",
        "        #set the attributes.\n",
        "        self.step_invt[self.current_step] = invt_power\n",
        "        self.step_rect[self.current_step] = rect_power\n",
        "\n",
        "    def tou_purchase_inc(self):\n",
        "        #Summer Months: 5.92, 2.09, 1.33\n",
        "        #Winter Months: 2.22, 1.66, 1.21\n",
        "        #un normalise the price for that step.\n",
        "        step_price = self.price_scaler.inverse_transform(self.purchase_price[self.current_step].reshape(-1,1))\n",
        "        #the conditions are a bit janky due to the different prices in Summer and Winter.\n",
        "        if step_price < 1.5:\n",
        "            self.off_peak_purchases[self.current_step] = self.step_purchased[self.current_step]\n",
        "            self.off_peak_cost[self.current_step]      = self.step_purchased[self.current_step]*step_price\n",
        "        elif step_price < 2.1:\n",
        "            self.standard_purchases[self.current_step] = self.step_purchased[self.current_step]\n",
        "            self.standard_cost[self.current_step]      = self.step_purchased[self.current_step]*step_price\n",
        "        else:\n",
        "            self.peak_purchases[self.current_step] = self.step_purchased[self.current_step]\n",
        "            self.peak_cost[self.current_step]      = self.step_purchased[self.current_step]*step_price\n",
        "\n",
        "    def AC_bus(self):\n",
        "        #fill out info on the ac\n",
        "        ac_gen = self.actual_gen[self.current_step, 0]\n",
        "        ac_load = self.actual_load[self.current_step,0]\n",
        "        ac_power_bal = ac_gen - ac_load\n",
        "        #check if there is load shedding or not\n",
        "        avail_grid = not self.load_shed[self.current_step]\n",
        "        #return relevant values\n",
        "        return ac_power_bal,avail_grid\n",
        "\n",
        "    def DC_bus(self):\n",
        "        #fill in info for DC_bus\n",
        "        dc_gen       = self.actual_gen[self.current_step,1]\n",
        "        dc_load      = self.actual_load[self.current_step,1]\n",
        "        dc_power_bal = dc_gen - dc_load\n",
        "        #fetch and transform information about the battery cap and availability\n",
        "        avail_bat  = self.power_scaler.inverse_transform(self.battery_level[self.current_step].reshape(-1,1)) - self.bat_threshold\n",
        "        avail_stor = self.bat_cap   - self.power_scaler.inverse_transform(self.battery_level[self.current_step].reshape(-1,1))\n",
        "        return dc_power_bal, avail_bat, avail_stor\n",
        "\n",
        "\n",
        "    def fill_load(self,actual_load):\n",
        "        if actual_load is None :#if actual_load is a string then fill with random numbers\n",
        "            self.actual_load = np.random.rand(self.final_step+self.num_preds+1,2).astype(np.float32)\n",
        "        else:                           #else fill the load with the correct number of entries from the input\n",
        "            self.actual_load  = actual_load[:self.episode_len,:]\n",
        "\n",
        "    def fill_gen(self,actual_gen):\n",
        "        if actual_gen is None:#if actual_gen is a string then fill with random numbers\n",
        "            self.actual_gen  = np.random.rand(self.final_step+self.num_preds+1,2).astype(np.float32)\n",
        "        else:                           #else fill the gen with the correct number of entries from the input\n",
        "            self.actual_gen  = actual_gen[:self.episode_len,:]\n",
        "\n",
        "    def fill_load_forecast(self, load_forecast):\n",
        "        if load_forecast is None:\n",
        "            load_sum = self.actual_load.sum(axis = 1)\n",
        "            self.load_forecast = [load_sum[i+1:i+1+self.num_preds] for i in range(self.final_step+1)]\n",
        "            self.load_forecast = np.array(self.load_forecast).astype(np.float32)\n",
        "        else:\n",
        "            self.load_forecast = np.array(load_forecast[:self.final_step,:]).astype(np.float32)\n",
        "\n",
        "    def fill_gen_forecast(self, gen_forecast):\n",
        "        if gen_forecast is None:\n",
        "            gen_sum = self.actual_gen.sum(axis = 1)\n",
        "            self.gen_forecast = [gen_sum[i+1:i+1+self.num_preds] for i in range(self.final_step+1)]\n",
        "            self.gen_forecast = np.array(self.gen_forecast).astype(np.float32)\n",
        "        else:\n",
        "            self.gen_forecast = np.array(gen_forecast[:self.final_step,:]).astype(np.float32)\n",
        "\n",
        "    def fill_power_bal(self):\n",
        "        actual_power_bal = self.actual_gen-self.actual_load\n",
        "        if len(actual_power_bal.shape) >= 2:\n",
        "            actual_power_bal = actual_power_bal.sum(axis=1)\n",
        "        self.power_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "        #in order for the agent to see the kw in the battery in proportion to the kw in power_bal observations. I have included the bat_cap in the transform, and the battery level will be transformed using the same scaler.\n",
        "        self.power_scaler.fit_transform(np.array(actual_power_bal,self.bat_cap).reshape(-1,1))\n",
        "        self.actual_power_bal = self.power_scaler.transform(actual_power_bal.reshape(-1,1))\n",
        "\n",
        "    def fill_price(self,purchase_price):\n",
        "        purchase_price = np.array(purchase_price).astype(np.float32)\n",
        "        if len(purchase_price)<self.episode_len: #if the proved set of purchase prices is less than the episode length, wrap it so it fills the episode.\n",
        "            repetitions    = (self.final_step+self.num_preds+1) // len(purchase_price)\n",
        "            remainder      = (self.final_step+self.num_preds+1) % len(purchase_price)\n",
        "            self.purchase_price =np.concatenate([purchase_price]*repetitions+[purchase_price[:remainder]])#need to read in from somewhere\n",
        "        else:                                    #else extract the correct number of prices.\n",
        "            self.purchase_price = purchase_price[:self.episode_len]\n",
        "\n",
        "        price_scaler = MinMaxScaler(feature_range=(0,1)) #scale purchase prices to 0,1\n",
        "        self.purchase_price = price_scaler.fit_transform(self.purchase_price.reshape(-1, 1))\n",
        "        self.price_scaler = price_scaler\n",
        "\n",
        "    def fill_shedding(self,load_shedding):\n",
        "        if load_shedding is None: #if no loadshedding schedule has been provided then create a array with between 2-5% instances of loadshedding\n",
        "            num_shedding   = np.random.randint(int(0.02*self.episode_len), int(0.05*self.episode_len))\n",
        "            load_shed      = np.array([1]*num_shedding + [0]*(self.episode_len - num_shedding))\n",
        "            np.random.shuffle(load_shed)\n",
        "            self.load_shed = load_shed\n",
        "        else:                            #else if load shedding schedule is provided collect the correct number of instances\n",
        "            self.load_shed = load_shedding[:self.episode_len]\n",
        "\n",
        "    def get_forecast(self):\n",
        "        load_forecast =   (self.load_forecast[self.current_step,:self.num_preds]).reshape(-1,1)\n",
        "        gen_forecast  =   (self.gen_forecast[self.current_step,:self.num_preds]).reshape(-1,1)\n",
        "        power_bal_forecast = gen_forecast - load_forecast\n",
        "        power_bal_forecast = self.power_scaler.transform(power_bal_forecast.reshape(-1,1))\n",
        "        return power_bal_forecast\n",
        "\n",
        "    def get_demand_charge(self):\n",
        "        if self.current_step % (30*24) == 0:\n",
        "            if self.current_step > 0:\n",
        "                max_demand = max(self.step_purchased[max(0,self.current_step - 30*24) : self.current_step])\n",
        "            else:\n",
        "                max_demand = 0\n",
        "            demand_charge = max_demand * self.demand_charge\n",
        "        else:\n",
        "            demand_charge = 0\n",
        "        return demand_charge\n",
        "\n",
        "    def get_demand_penalty(self):\n",
        "        if self.current_step>0:\n",
        "            rolling_peak_demand = max(self.step_purchased[max(0, self.current_step-30*24):self.current_step])\n",
        "        else:\n",
        "            rolling_peak_demand = 0\n",
        "        rolling_peak_demand = 300\n",
        "        #demand_penalty = - ( self.step_purchased[self.current_step] - rolling_peak_demand)\n",
        "        demand_penalty = np.exp(self.action_scaler.transform(self.step_purchased[self.current_step].reshape(-1,1)) - self.action_scaler.transform(np.array(rolling_peak_demand).reshape(-1,1)))*200\n",
        "        return demand_penalty\n",
        "#define a pointer (kinda, don't actually know what its called) a\n",
        "EMS = EMSv2_7\n",
        "\n",
        "# Register environment so I can use make_vec_env\n",
        "register(\n",
        "# unique identifier for the env `name-version`\n",
        "id=f\"{version}\",\n",
        "# path to the class for creating the env\n",
        "# Note: entry_point also accept a class as input (and not only a string)\n",
        "entry_point= EMS(),\n",
        "\n",
        ")\n",
        "\n",
        "#Check the environment with stable_baselines3 check_env.\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "env = EMS()\n",
        "check_env(env,warn = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG4gB2cM7tDY"
      },
      "source": [
        "**Load in the data for our specific microgrid.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0X9RBdXC_2PD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398a4c17-6539-4647-b083-14cf5919684f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reset observation space looks like: [ 0.07764236 -0.49452567  1.          0.          0.          0.\n",
            " -0.53609806 -0.528026   -0.45572138  0.          0.          0.\n",
            "  0.        ]\n",
            "After action 0: \n",
            "Battery level is: 0.07764235883951187kWh\n",
            "Current  Power Balance -0.494525671005249\n",
            "Forecasted  power bal 1 hour ahead: -0.536098062992096.\n",
            "Forecasted  power bal 2 hour ahead: -0.5280259847640991. \n",
            "Forecasted  power bal 3 hour ahead: -0.455721378326416. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.0. \n",
            "Forecasted  price 3 hour ahead: 0.0. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "After action 0: \n",
            "Battery level is: -0.07079396396875381kWh\n",
            "Current  Power Balance -0.536098062992096\n",
            "Forecasted  power bal 1 hour ahead: -0.5280259847640991.\n",
            "Forecasted  power bal 2 hour ahead: -0.455721378326416. \n",
            "Forecasted  power bal 3 hour ahead: -0.31690630316734314. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.0. \n",
            "Forecasted  price 3 hour ahead: 0.0. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "After action 0: \n",
            "Battery level is: -0.17659667134284973kWh\n",
            "Current  Power Balance -0.5280259847640991\n",
            "Forecasted  power bal 1 hour ahead: -0.455721378326416.\n",
            "Forecasted  power bal 2 hour ahead: -0.31690630316734314. \n",
            "Forecasted  power bal 3 hour ahead: -0.1943129003047943. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.0. \n",
            "Forecasted  price 3 hour ahead: 0.0. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "After action 0: \n",
            "Battery level is: -0.17659667134284973kWh\n",
            "Current  Power Balance -0.455721378326416\n",
            "Forecasted  power bal 1 hour ahead: -0.31690630316734314.\n",
            "Forecasted  power bal 2 hour ahead: -0.1943129003047943. \n",
            "Forecasted  power bal 3 hour ahead: 0.09753653407096863. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.0. \n",
            "Forecasted  price 3 hour ahead: 0.09576204419136047. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "After action 1: \n",
            "Battery level is: 0.5013740658760071kWh\n",
            "Current  Power Balance -0.31690627336502075\n",
            "Forecasted  power bal 1 hour ahead: -0.1943129003047943.\n",
            "Forecasted  power bal 2 hour ahead: 0.09753653407096863. \n",
            "Forecasted  power bal 3 hour ahead: 0.2722266912460327. \n",
            "current Purchase Prcie: 0.0.\n",
            "Forecasted  price 1 hour ahead: 0.0. \n",
            "Forecasted  price 2 hour ahead: 0.09576204419136047. \n",
            "Forecasted  price 3 hour ahead: 0.21427208185195923. \n",
            "_________________________________________________________________________________________________________________\n",
            "\n",
            "Done iteration! Total reward accumulated is: -1577201.5967719331\n"
          ]
        }
      ],
      "source": [
        "#need to import data from Github\n",
        "path_data = \"/content/EEE4022S_BNKJUL001_Thesis/PythonWorkspace/dataClean.csv\"\n",
        "data = pd.read_csv(path_data)\n",
        "\n",
        "path_pv_gen = \"/content/EEE4022S_BNKJUL001_Thesis/Generation/BNKJUL001_Thesis_solarGen500kWHomer.csv\"\n",
        "data_pv_gen = pd.read_csv(path_pv_gen)\n",
        "\n",
        "path_wind_gen = \"/content/EEE4022S_BNKJUL001_Thesis/Generation/BNKJUL001_Thesis_Wind500kGenHomer.csv\"\n",
        "data_wind_gen = pd.read_csv(path_wind_gen)\n",
        "\n",
        "\n",
        "path_shedding = \"/content/EEE4022S_BNKJUL001_Thesis/MatlabWorkSpace/loadShedding2022.csv\"\n",
        "data_shedding = pd.read_csv(path_shedding)\n",
        "load_shedding = data_shedding['LoadShedding'].values.astype(np.float32)\n",
        "\n",
        "#load_forecasts = scipy.io.loadmat('load_forecasts.mat')\n",
        "#load_forecasts = load_forecasts['load_forecasts']\n",
        "\n",
        "#get the correct arrays from the data\n",
        "wind_gen = data_wind_gen['Wind_Out'].values.astype(np.float32)\n",
        "PV_gen = data_pv_gen['PV_Out'].values.astype(np.float32)\n",
        "actual_gen = np.column_stack((wind_gen, PV_gen))\n",
        "#read in ac and DC load\n",
        "AC_load = data['AC'].values.astype(np.float32)\n",
        "DC_load = data['DC'].values.astype(np.float32)\n",
        "#stack em together for the input :)\n",
        "actual_load = np.column_stack((AC_load, DC_load))\n",
        "\n",
        "path_purchase_price = \"/content/EEE4022S_BNKJUL001_Thesis/PythonWorkspace/purchasePrice.csv\"\n",
        "data_purchase_price = pd.read_csv(path_purchase_price)\n",
        "purchase_price = data_purchase_price['Grid Power Price'].values.astype(np.float32)\n",
        "\n",
        "#define the base environment\n",
        "base_env = EMS(episode_len = 6000, actual_load = actual_load, actual_gen = actual_gen, bat_threshold = 100, bat_cap = 500, purchase_price = purchase_price,num_preds = 3)\n",
        "#going to print out a bunch of things to test the different spaces.\n",
        "obs,_    = base_env.reset()\n",
        "\n",
        "def print_obs(action_standby=0):\n",
        "    print(f\"After action {action_standby}: \" )\n",
        "    battery_level = obs[0]\n",
        "    print(f\"Battery level is: {battery_level}kWh\")\n",
        "    current_power_bal = obs[1]\n",
        "    print(f\"Current  Power Balance {current_power_bal}\")\n",
        "    power_forecast = obs[6:9]\n",
        "    print(f\"Forecasted  power bal 1 hour ahead: {power_forecast[0]}.\")\n",
        "    print(f\"Forecasted  power bal 2 hour ahead: {power_forecast[1]}. \")\n",
        "    print(f\"Forecasted  power bal 3 hour ahead: {power_forecast[2]}. \")\n",
        "    price = obs[9:]\n",
        "    print(f\"current Purchase Prcie: {price[0]}.\")\n",
        "    print(f\"Forecasted  price 1 hour ahead: {price[1]}. \")\n",
        "    print(f\"Forecasted  price 2 hour ahead: {price[2]}. \")\n",
        "    print(f\"Forecasted  price 3 hour ahead: {price[3]}. \")\n",
        "    print(f\"_________________________________________________________________________________________________________________\")\n",
        "    print(f\"\")\n",
        "\n",
        "print(f\"The reset observation space looks like: {obs}\")\n",
        "\n",
        "print_obs()\n",
        "\n",
        "action_standby = -1\n",
        "obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "\n",
        "print_obs()\n",
        "\n",
        "obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "\n",
        "print_obs()\n",
        "\n",
        "\n",
        "obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "print_obs()\n",
        "\n",
        "action_standby = 1\n",
        "obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "print_obs(action_standby)\n",
        "\n",
        "#Evaluate the base model (no EMS, just using standby mode)\n",
        "#reset score\n",
        "score = 0\n",
        "obs,_    = base_env.reset()\n",
        "#ensure that the exit condition is reset\n",
        "truncated = False\n",
        "#define the action to take. -1 represents requesting to buy 0kw every step.\n",
        "action_standby = -1\n",
        "\n",
        "while not truncated:\n",
        "    obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "    score += reward\n",
        "\n",
        "print(f\"Done iteration! Total reward accumulated is: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pAj6-n04zyt"
      },
      "source": [
        "**LOAD OR MAKE MODEL HERE!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gQDhUNGWeum4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47adf9c-6aae-4ff7-a197-8910144f6e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 595         |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 1685        |\n",
            "|    total_timesteps      | 1003520     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020947684 |\n",
            "|    clip_fraction        | 0.189       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.762       |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 8.94e+04    |\n",
            "|    n_updates            | 3910        |\n",
            "|    policy_gradient_loss | 0.00593     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 7.37e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 595         |\n",
            "|    iterations           | 99          |\n",
            "|    time_elapsed         | 1702        |\n",
            "|    total_timesteps      | 1013760     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018762568 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.758       |\n",
            "|    explained_variance   | 0.757       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.3e+05     |\n",
            "|    n_updates            | 3920        |\n",
            "|    policy_gradient_loss | 0.00472     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 8.32e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 595         |\n",
            "|    iterations           | 100         |\n",
            "|    time_elapsed         | 1718        |\n",
            "|    total_timesteps      | 1024000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018415183 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.754       |\n",
            "|    explained_variance   | 0.832       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.04e+05    |\n",
            "|    n_updates            | 3930        |\n",
            "|    policy_gradient_loss | 0.00127     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 1.25e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 595         |\n",
            "|    iterations           | 101         |\n",
            "|    time_elapsed         | 1735        |\n",
            "|    total_timesteps      | 1034240     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028548742 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.762       |\n",
            "|    explained_variance   | 0.892       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.61e+05    |\n",
            "|    n_updates            | 3940        |\n",
            "|    policy_gradient_loss | 0.00587     |\n",
            "|    std                  | 0.113       |\n",
            "|    value_loss           | 7.65e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 595         |\n",
            "|    iterations           | 102         |\n",
            "|    time_elapsed         | 1753        |\n",
            "|    total_timesteps      | 1044480     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020926483 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.761       |\n",
            "|    explained_variance   | 0.72        |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.45e+05    |\n",
            "|    n_updates            | 3950        |\n",
            "|    policy_gradient_loss | 0.00432     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 8.48e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 103         |\n",
            "|    time_elapsed         | 1769        |\n",
            "|    total_timesteps      | 1054720     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017910548 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.755       |\n",
            "|    explained_variance   | 0.837       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.73e+05    |\n",
            "|    n_updates            | 3960        |\n",
            "|    policy_gradient_loss | 0.00231     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 1.17e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 104         |\n",
            "|    time_elapsed         | 1786        |\n",
            "|    total_timesteps      | 1064960     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022163114 |\n",
            "|    clip_fraction        | 0.183       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.751       |\n",
            "|    explained_variance   | 0.874       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.2e+05     |\n",
            "|    n_updates            | 3970        |\n",
            "|    policy_gradient_loss | 0.00578     |\n",
            "|    std                  | 0.113       |\n",
            "|    value_loss           | 6.36e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 105         |\n",
            "|    time_elapsed         | 1803        |\n",
            "|    total_timesteps      | 1075200     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018904123 |\n",
            "|    clip_fraction        | 0.167       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.781       |\n",
            "|    explained_variance   | 0.831       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.56e+05    |\n",
            "|    n_updates            | 3980        |\n",
            "|    policy_gradient_loss | 0.00324     |\n",
            "|    std                  | 0.11        |\n",
            "|    value_loss           | 8.44e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.97e+03   |\n",
            "|    ep_rew_mean          | -1.4e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 596        |\n",
            "|    iterations           | 106        |\n",
            "|    time_elapsed         | 1820       |\n",
            "|    total_timesteps      | 1085440    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01998287 |\n",
            "|    clip_fraction        | 0.159      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.787      |\n",
            "|    explained_variance   | 0.856      |\n",
            "|    learning_rate        | 0.01       |\n",
            "|    loss                 | 2.58e+05   |\n",
            "|    n_updates            | 3990       |\n",
            "|    policy_gradient_loss | 0.00269    |\n",
            "|    std                  | 0.111      |\n",
            "|    value_loss           | 1.3e+06    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 107         |\n",
            "|    time_elapsed         | 1837        |\n",
            "|    total_timesteps      | 1095680     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027139306 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.77        |\n",
            "|    explained_variance   | 0.837       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.49e+05    |\n",
            "|    n_updates            | 4000        |\n",
            "|    policy_gradient_loss | 0.00921     |\n",
            "|    std                  | 0.113       |\n",
            "|    value_loss           | 5.82e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 108         |\n",
            "|    time_elapsed         | 1853        |\n",
            "|    total_timesteps      | 1105920     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018683963 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.773       |\n",
            "|    explained_variance   | 0.851       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.44e+05    |\n",
            "|    n_updates            | 4010        |\n",
            "|    policy_gradient_loss | 0.00481     |\n",
            "|    std                  | 0.112       |\n",
            "|    value_loss           | 8.71e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 109         |\n",
            "|    time_elapsed         | 1870        |\n",
            "|    total_timesteps      | 1116160     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027485048 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.77        |\n",
            "|    explained_variance   | 0.901       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.18e+05    |\n",
            "|    n_updates            | 4020        |\n",
            "|    policy_gradient_loss | 0.00248     |\n",
            "|    std                  | 0.112       |\n",
            "|    value_loss           | 1.2e+06     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 110         |\n",
            "|    time_elapsed         | 1887        |\n",
            "|    total_timesteps      | 1126400     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023668444 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.763       |\n",
            "|    explained_variance   | 0.836       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 9.92e+04    |\n",
            "|    n_updates            | 4030        |\n",
            "|    policy_gradient_loss | 0.00715     |\n",
            "|    std                  | 0.115       |\n",
            "|    value_loss           | 6.05e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 111         |\n",
            "|    time_elapsed         | 1904        |\n",
            "|    total_timesteps      | 1136640     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018923448 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.745       |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.11e+05    |\n",
            "|    n_updates            | 4040        |\n",
            "|    policy_gradient_loss | 0.00505     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 9.7e+05     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 112         |\n",
            "|    time_elapsed         | 1921        |\n",
            "|    total_timesteps      | 1146880     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020756578 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.751       |\n",
            "|    explained_variance   | 0.914       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.95e+05    |\n",
            "|    n_updates            | 4050        |\n",
            "|    policy_gradient_loss | 0.00365     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 1.12e+06    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.97e+03   |\n",
            "|    ep_rew_mean          | -1.4e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 597        |\n",
            "|    iterations           | 113        |\n",
            "|    time_elapsed         | 1937       |\n",
            "|    total_timesteps      | 1157120    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02267025 |\n",
            "|    clip_fraction        | 0.179      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.76       |\n",
            "|    explained_variance   | 0.844      |\n",
            "|    learning_rate        | 0.01       |\n",
            "|    loss                 | 9.53e+04   |\n",
            "|    n_updates            | 4060       |\n",
            "|    policy_gradient_loss | 0.00441    |\n",
            "|    std                  | 0.113      |\n",
            "|    value_loss           | 6.42e+05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 597         |\n",
            "|    iterations           | 114         |\n",
            "|    time_elapsed         | 1954        |\n",
            "|    total_timesteps      | 1167360     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022029232 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.77        |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.81e+05    |\n",
            "|    n_updates            | 4070        |\n",
            "|    policy_gradient_loss | 0.00548     |\n",
            "|    std                  | 0.112       |\n",
            "|    value_loss           | 1.04e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 597         |\n",
            "|    iterations           | 115         |\n",
            "|    time_elapsed         | 1971        |\n",
            "|    total_timesteps      | 1177600     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019500718 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.763       |\n",
            "|    explained_variance   | 0.913       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.52e+05    |\n",
            "|    n_updates            | 4080        |\n",
            "|    policy_gradient_loss | 0.0038      |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 1.08e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 597         |\n",
            "|    iterations           | 116         |\n",
            "|    time_elapsed         | 1988        |\n",
            "|    total_timesteps      | 1187840     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020279761 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.759       |\n",
            "|    explained_variance   | 0.857       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.3e+05     |\n",
            "|    n_updates            | 4090        |\n",
            "|    policy_gradient_loss | 0.00601     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 6.42e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.97e+03   |\n",
            "|    ep_rew_mean          | -1.4e+06   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 597        |\n",
            "|    iterations           | 117        |\n",
            "|    time_elapsed         | 2005       |\n",
            "|    total_timesteps      | 1198080    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01915077 |\n",
            "|    clip_fraction        | 0.158      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.755      |\n",
            "|    explained_variance   | 0.873      |\n",
            "|    learning_rate        | 0.01       |\n",
            "|    loss                 | 1.82e+05   |\n",
            "|    n_updates            | 4100       |\n",
            "|    policy_gradient_loss | 0.00227    |\n",
            "|    std                  | 0.114      |\n",
            "|    value_loss           | 1.01e+06   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 597         |\n",
            "|    iterations           | 118         |\n",
            "|    time_elapsed         | 2021        |\n",
            "|    total_timesteps      | 1208320     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024332112 |\n",
            "|    clip_fraction        | 0.18        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.758       |\n",
            "|    explained_variance   | 0.92        |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.84e+05    |\n",
            "|    n_updates            | 4110        |\n",
            "|    policy_gradient_loss | 0.00467     |\n",
            "|    std                  | 0.113       |\n",
            "|    value_loss           | 1.05e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.4e+06    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 597         |\n",
            "|    iterations           | 119         |\n",
            "|    time_elapsed         | 2038        |\n",
            "|    total_timesteps      | 1218560     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027447691 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.769       |\n",
            "|    explained_variance   | 0.861       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.18e+05    |\n",
            "|    n_updates            | 4120        |\n",
            "|    policy_gradient_loss | 0.00541     |\n",
            "|    std                  | 0.112       |\n",
            "|    value_loss           | 6.59e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 597         |\n",
            "|    iterations           | 120         |\n",
            "|    time_elapsed         | 2056        |\n",
            "|    total_timesteps      | 1228800     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019610211 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.772       |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.24e+05    |\n",
            "|    n_updates            | 4130        |\n",
            "|    policy_gradient_loss | 0.00406     |\n",
            "|    std                  | 0.112       |\n",
            "|    value_loss           | 1.07e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 597         |\n",
            "|    iterations           | 121         |\n",
            "|    time_elapsed         | 2072        |\n",
            "|    total_timesteps      | 1239040     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023892108 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.77        |\n",
            "|    explained_variance   | 0.923       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.64e+05    |\n",
            "|    n_updates            | 4140        |\n",
            "|    policy_gradient_loss | 0.00364     |\n",
            "|    std                  | 0.112       |\n",
            "|    value_loss           | 1.01e+06    |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 5.97e+03  |\n",
            "|    ep_rew_mean          | -1.39e+06 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 597       |\n",
            "|    iterations           | 122       |\n",
            "|    time_elapsed         | 2089      |\n",
            "|    total_timesteps      | 1249280   |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0206339 |\n",
            "|    clip_fraction        | 0.187     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.769     |\n",
            "|    explained_variance   | 0.838     |\n",
            "|    learning_rate        | 0.01      |\n",
            "|    loss                 | 7.53e+04  |\n",
            "|    n_updates            | 4150      |\n",
            "|    policy_gradient_loss | 0.00612   |\n",
            "|    std                  | 0.113     |\n",
            "|    value_loss           | 7.06e+05  |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 123         |\n",
            "|    time_elapsed         | 2106        |\n",
            "|    total_timesteps      | 1259520     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016164083 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.758       |\n",
            "|    explained_variance   | 0.86        |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.21e+05    |\n",
            "|    n_updates            | 4160        |\n",
            "|    policy_gradient_loss | 0.00453     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 1.04e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 597         |\n",
            "|    iterations           | 124         |\n",
            "|    time_elapsed         | 2123        |\n",
            "|    total_timesteps      | 1269760     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021376228 |\n",
            "|    clip_fraction        | 0.178       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.752       |\n",
            "|    explained_variance   | 0.925       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.19e+05    |\n",
            "|    n_updates            | 4170        |\n",
            "|    policy_gradient_loss | 0.00448     |\n",
            "|    std                  | 0.115       |\n",
            "|    value_loss           | 9.85e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 125         |\n",
            "|    time_elapsed         | 2140        |\n",
            "|    total_timesteps      | 1280000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016217299 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.756       |\n",
            "|    explained_variance   | 0.841       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 8.44e+04    |\n",
            "|    n_updates            | 4180        |\n",
            "|    policy_gradient_loss | 0.00449     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 7.15e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 126         |\n",
            "|    time_elapsed         | 2156        |\n",
            "|    total_timesteps      | 1290240     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015692234 |\n",
            "|    clip_fraction        | 0.153       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.754       |\n",
            "|    explained_variance   | 0.873       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.36e+05    |\n",
            "|    n_updates            | 4190        |\n",
            "|    policy_gradient_loss | 0.00332     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 1.1e+06     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 127         |\n",
            "|    time_elapsed         | 2173        |\n",
            "|    total_timesteps      | 1300480     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023705129 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.752       |\n",
            "|    explained_variance   | 0.925       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.39e+05    |\n",
            "|    n_updates            | 4200        |\n",
            "|    policy_gradient_loss | 0.00438     |\n",
            "|    std                  | 0.115       |\n",
            "|    value_loss           | 9.11e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 128         |\n",
            "|    time_elapsed         | 2189        |\n",
            "|    total_timesteps      | 1310720     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024714526 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.738       |\n",
            "|    explained_variance   | 0.816       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.02e+05    |\n",
            "|    n_updates            | 4210        |\n",
            "|    policy_gradient_loss | 0.00554     |\n",
            "|    std                  | 0.118       |\n",
            "|    value_loss           | 7.35e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 129         |\n",
            "|    time_elapsed         | 2207        |\n",
            "|    total_timesteps      | 1320960     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018701818 |\n",
            "|    clip_fraction        | 0.156       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.729       |\n",
            "|    explained_variance   | 0.875       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.11e+05    |\n",
            "|    n_updates            | 4220        |\n",
            "|    policy_gradient_loss | 0.0011      |\n",
            "|    std                  | 0.117       |\n",
            "|    value_loss           | 1.11e+06    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.97e+03   |\n",
            "|    ep_rew_mean          | -1.39e+06  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 598        |\n",
            "|    iterations           | 130        |\n",
            "|    time_elapsed         | 2224       |\n",
            "|    total_timesteps      | 1331200    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02122806 |\n",
            "|    clip_fraction        | 0.169      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.736      |\n",
            "|    explained_variance   | 0.927      |\n",
            "|    learning_rate        | 0.01       |\n",
            "|    loss                 | 1.34e+05   |\n",
            "|    n_updates            | 4230       |\n",
            "|    policy_gradient_loss | 0.00292    |\n",
            "|    std                  | 0.115      |\n",
            "|    value_loss           | 8.63e+05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 131         |\n",
            "|    time_elapsed         | 2240        |\n",
            "|    total_timesteps      | 1341440     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016505452 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.745       |\n",
            "|    explained_variance   | 0.77        |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.04e+05    |\n",
            "|    n_updates            | 4240        |\n",
            "|    policy_gradient_loss | 0.00375     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 7.74e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 132         |\n",
            "|    time_elapsed         | 2257        |\n",
            "|    total_timesteps      | 1351680     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017040601 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.753       |\n",
            "|    explained_variance   | 0.86        |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.1e+05     |\n",
            "|    n_updates            | 4250        |\n",
            "|    policy_gradient_loss | 0.00404     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 1.24e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 133         |\n",
            "|    time_elapsed         | 2275        |\n",
            "|    total_timesteps      | 1361920     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024135273 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.748       |\n",
            "|    explained_variance   | 0.916       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.44e+05    |\n",
            "|    n_updates            | 4260        |\n",
            "|    policy_gradient_loss | 0.00519     |\n",
            "|    std                  | 0.115       |\n",
            "|    value_loss           | 6.82e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 134         |\n",
            "|    time_elapsed         | 2291        |\n",
            "|    total_timesteps      | 1372160     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017955627 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.756       |\n",
            "|    explained_variance   | 0.762       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1e+05       |\n",
            "|    n_updates            | 4270        |\n",
            "|    policy_gradient_loss | 0.00466     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 7.65e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 599         |\n",
            "|    iterations           | 135         |\n",
            "|    time_elapsed         | 2307        |\n",
            "|    total_timesteps      | 1382400     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019408824 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.751       |\n",
            "|    explained_variance   | 0.848       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.77e+05    |\n",
            "|    n_updates            | 4280        |\n",
            "|    policy_gradient_loss | 0.00354     |\n",
            "|    std                  | 0.114       |\n",
            "|    value_loss           | 1.2e+06     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.97e+03   |\n",
            "|    ep_rew_mean          | -1.39e+06  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 599        |\n",
            "|    iterations           | 136        |\n",
            "|    time_elapsed         | 2324       |\n",
            "|    total_timesteps      | 1392640    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03120274 |\n",
            "|    clip_fraction        | 0.196      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.757      |\n",
            "|    explained_variance   | 0.905      |\n",
            "|    learning_rate        | 0.01       |\n",
            "|    loss                 | 1.05e+05   |\n",
            "|    n_updates            | 4290       |\n",
            "|    policy_gradient_loss | 0.00686    |\n",
            "|    std                  | 0.113      |\n",
            "|    value_loss           | 6.68e+05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 599         |\n",
            "|    iterations           | 137         |\n",
            "|    time_elapsed         | 2340        |\n",
            "|    total_timesteps      | 1402880     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021325225 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.755       |\n",
            "|    explained_variance   | 0.73        |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 8.57e+04    |\n",
            "|    n_updates            | 4300        |\n",
            "|    policy_gradient_loss | 0.00298     |\n",
            "|    std                  | 0.115       |\n",
            "|    value_loss           | 7.87e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 599         |\n",
            "|    iterations           | 138         |\n",
            "|    time_elapsed         | 2358        |\n",
            "|    total_timesteps      | 1413120     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022587894 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.746       |\n",
            "|    explained_variance   | 0.85        |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.79e+05    |\n",
            "|    n_updates            | 4310        |\n",
            "|    policy_gradient_loss | 0.00404     |\n",
            "|    std                  | 0.115       |\n",
            "|    value_loss           | 1.17e+06    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.97e+03   |\n",
            "|    ep_rew_mean          | -1.39e+06  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 599        |\n",
            "|    iterations           | 139        |\n",
            "|    time_elapsed         | 2374       |\n",
            "|    total_timesteps      | 1423360    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02765156 |\n",
            "|    clip_fraction        | 0.182      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.735      |\n",
            "|    explained_variance   | 0.895      |\n",
            "|    learning_rate        | 0.01       |\n",
            "|    loss                 | 7.99e+04   |\n",
            "|    n_updates            | 4320       |\n",
            "|    policy_gradient_loss | 0.00656    |\n",
            "|    std                  | 0.117      |\n",
            "|    value_loss           | 5.82e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.97e+03   |\n",
            "|    ep_rew_mean          | -1.39e+06  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 599        |\n",
            "|    iterations           | 140        |\n",
            "|    time_elapsed         | 2391       |\n",
            "|    total_timesteps      | 1433600    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01759709 |\n",
            "|    clip_fraction        | 0.16       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.72       |\n",
            "|    explained_variance   | 0.841      |\n",
            "|    learning_rate        | 0.01       |\n",
            "|    loss                 | 1.01e+05   |\n",
            "|    n_updates            | 4330       |\n",
            "|    policy_gradient_loss | 0.00304    |\n",
            "|    std                  | 0.118      |\n",
            "|    value_loss           | 8.04e+05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 599         |\n",
            "|    iterations           | 141         |\n",
            "|    time_elapsed         | 2407        |\n",
            "|    total_timesteps      | 1443840     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017447874 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.734       |\n",
            "|    explained_variance   | 0.863       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 2.74e+05    |\n",
            "|    n_updates            | 4340        |\n",
            "|    policy_gradient_loss | 0.00176     |\n",
            "|    std                  | 0.115       |\n",
            "|    value_loss           | 1.25e+06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 599         |\n",
            "|    iterations           | 142         |\n",
            "|    time_elapsed         | 2424        |\n",
            "|    total_timesteps      | 1454080     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020740915 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.747       |\n",
            "|    explained_variance   | 0.853       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.11e+05    |\n",
            "|    n_updates            | 4350        |\n",
            "|    policy_gradient_loss | 0.00621     |\n",
            "|    std                  | 0.115       |\n",
            "|    value_loss           | 5.59e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.97e+03   |\n",
            "|    ep_rew_mean          | -1.39e+06  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 599        |\n",
            "|    iterations           | 143        |\n",
            "|    time_elapsed         | 2441       |\n",
            "|    total_timesteps      | 1464320    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01786692 |\n",
            "|    clip_fraction        | 0.161      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.748      |\n",
            "|    explained_variance   | 0.864      |\n",
            "|    learning_rate        | 0.01       |\n",
            "|    loss                 | 1.15e+05   |\n",
            "|    n_updates            | 4360       |\n",
            "|    policy_gradient_loss | 0.00392    |\n",
            "|    std                  | 0.115      |\n",
            "|    value_loss           | 8.72e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.97e+03   |\n",
            "|    ep_rew_mean          | -1.39e+06  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 599        |\n",
            "|    iterations           | 144        |\n",
            "|    time_elapsed         | 2458       |\n",
            "|    total_timesteps      | 1474560    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02096178 |\n",
            "|    clip_fraction        | 0.156      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.751      |\n",
            "|    explained_variance   | 0.907      |\n",
            "|    learning_rate        | 0.01       |\n",
            "|    loss                 | 1.4e+05    |\n",
            "|    n_updates            | 4370       |\n",
            "|    policy_gradient_loss | 0.00387    |\n",
            "|    std                  | 0.114      |\n",
            "|    value_loss           | 1.12e+06   |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 5.97e+03  |\n",
            "|    ep_rew_mean          | -1.39e+06 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 599       |\n",
            "|    iterations           | 145       |\n",
            "|    time_elapsed         | 2474      |\n",
            "|    total_timesteps      | 1484800   |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0264231 |\n",
            "|    clip_fraction        | 0.189     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.759     |\n",
            "|    explained_variance   | 0.836     |\n",
            "|    learning_rate        | 0.01      |\n",
            "|    loss                 | 7.51e+04  |\n",
            "|    n_updates            | 4380      |\n",
            "|    policy_gradient_loss | 0.00824   |\n",
            "|    std                  | 0.113     |\n",
            "|    value_loss           | 5.58e+05  |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 600         |\n",
            "|    iterations           | 146         |\n",
            "|    time_elapsed         | 2491        |\n",
            "|    total_timesteps      | 1495040     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021131124 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.762       |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 1.16e+05    |\n",
            "|    n_updates            | 4390        |\n",
            "|    policy_gradient_loss | 0.00494     |\n",
            "|    std                  | 0.113       |\n",
            "|    value_loss           | 9.22e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97e+03    |\n",
            "|    ep_rew_mean          | -1.39e+06   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 599         |\n",
            "|    iterations           | 147         |\n",
            "|    time_elapsed         | 2508        |\n",
            "|    total_timesteps      | 1505280     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019158212 |\n",
            "|    clip_fraction        | 0.172       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.765       |\n",
            "|    explained_variance   | 0.914       |\n",
            "|    learning_rate        | 0.01        |\n",
            "|    loss                 | 3.26e+05    |\n",
            "|    n_updates            | 4400        |\n",
            "|    policy_gradient_loss | 0.00379     |\n",
            "|    std                  | 0.113       |\n",
            "|    value_loss           | 1.09e+06    |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model_note = \"\"\n",
        "config = {\n",
        "    \"policy_type\": \"MlpPolicy\",\n",
        "    \"total_timesteps\": 1_500_000,\n",
        "    \"notes\": f\"{model_note}_train\"\n",
        "}\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"4022_intelligent_ems\",\n",
        "    config=config,\n",
        "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    save_code=True,  # optional\n",
        ")\n",
        "\n",
        "#define 5 environments   for training and eval\n",
        "n_envs = 5\n",
        "#define num predictions\n",
        "n_preds = 24\n",
        "\n",
        "train_args = {\n",
        "                \"episode_len\"   : 6000,\n",
        "                \"actual_load\"   : actual_load,\n",
        "                \"actual_gen\"    : actual_gen,\n",
        "                \"bat_threshold\" : 100,\n",
        "                \"bat_cap\"       : 500,\n",
        "                \"purchase_price\": purchase_price,\n",
        "                \"num_preds\"     : n_preds,\n",
        "                \"load_shedding\" : load_shedding[2760:],\n",
        "                \"render_mode\"   : \"rgb_array\",\n",
        "                \"wandb_log\"     : True,\n",
        "                \"train_log\"     : True,\n",
        "\n",
        "                }\n",
        "\n",
        "eval_args = {\n",
        "                \"episode_len\"   :2760,\n",
        "                \"actual_load\"   :actual_load[6001:],\n",
        "                \"actual_gen\"    :actual_gen[6001:],\n",
        "                \"bat_threshold\" :100,\n",
        "                \"bat_cap\"       :500,\n",
        "                \"purchase_price\":purchase_price[6001:],\n",
        "                \"num_preds\"     :n_preds,\n",
        "                \"load_shedding\" :load_shedding[6001:],\n",
        "                \"wandb_log\"     : True,\n",
        "                \"train_log\"     : False\n",
        "}\n",
        "\n",
        "\n",
        "train_env = make_vec_env(EMS, n_envs = n_envs,env_kwargs = train_args )\n",
        "\n",
        "\n",
        "eval_env = make_vec_env(EMS, n_envs = n_envs,env_kwargs = eval_args )\n",
        "\n",
        "\n",
        "wand_eval = f\"{version}_{model_type}_eval\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "wand_train = f\"{version}_{model_type}_train\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "wandb_callback = WandbCallback(\n",
        "                gradient_save_freq=100,\n",
        "                model_save_path=f\"models/{run.id}.{datetime.datetime.now()}\",\n",
        "                model_save_freq= 30000,\n",
        "                verbose=2,\n",
        "                log = \"all\",\n",
        "               )\n",
        "eval_callback = EvalCallback(eval_env,\n",
        "                             best_model_save_path = f\"{model_dir}{version}_{model_type}\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "                             log_path = wand_eval,\n",
        "                             eval_freq=300,\n",
        "                             n_eval_episodes = 1,\n",
        "                             deterministic = True,\n",
        "                             render = False,\n",
        "                             callback_after_eval = wandb_callback)\n",
        "\n",
        "model_load = \"/content/drive/MyDrive/Colab Notebooks/EMSv2_7/models/PPO/EMSv2_7_PPO1018-221629.zip\"\n",
        "model  = PPO.load(model_load, env = train_env)\n",
        "#model = PPO(\"MlpPolicy\",train_env, verbose = 1, learning_rate = 0.01, vf_coef = 0.2,tensorboard_log = f\"runs/{run.id}\")#learning_rate = 0.01, vf_coef = 0.2\n",
        "\n",
        "model.learn(total_timesteps= config[\"total_timesteps\"],\n",
        "            tb_log_name = wand_train,\n",
        "            callback = wandb_callback\n",
        "            )\n",
        "\n",
        "model.save(f\"{model_dir}{version}_{model_type}\"+datetime.datetime.now().strftime(\"%m%d-%H%M%S\"))\n",
        "\n",
        "#c163c28885695cb2b0493ac455e296dcc8bc462a"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfcjwsYb2-oS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a new test environment and load up the best performing model to test it.**"
      ],
      "metadata": {
        "id": "C9AGcLlsv4N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "FBI9vEwCd76a"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "inDHszZaxBUS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f2f2a6a4281f43649addd9d2aa96eb08",
            "593de50190b8464893b1fdd0d2e67fc3",
            "310336a9b33148b7a018647db155558c",
            "0c75d94d374846f6a212f2b8e586cd9f",
            "ed0fbdc7e69e427997bf962a92318d07",
            "9417e447e4ac4073b3707be2e7392fbc",
            "8b0a881b4a75405cb15f7bebcdd2e66c",
            "9ce16f05fcbe4b08826965a2815104f2"
          ]
        },
        "outputId": "5b45fbe8-6f8b-47bb-f07b-db0b76c05b2a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:5qjbfp0i) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='1.354 MB of 1.354 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2f2a6a4281f43649addd9d2aa96eb08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Diesel Generator</td><td></td></tr><tr><td>Excess Generation</td><td></td></tr><tr><td>Inverter total power flow</td><td></td></tr><tr><td>Max Demand</td><td></td></tr><tr><td>Num Diesel Gen Actions</td><td></td></tr><tr><td>Num Off-Peak Purchases</td><td></td></tr><tr><td>Num Peak Purchases</td><td></td></tr><tr><td>Num Standard Purchases</td><td></td></tr><tr><td>Off-Peak Purchases</td><td></td></tr><tr><td>Peak Purchases</td><td></td></tr><tr><td>Rectifier total power flow</td><td></td></tr><tr><td>Standard Purchases</td><td></td></tr><tr><td>Total Elec Purchase</td><td></td></tr><tr><td>Total Money spent</td><td></td></tr><tr><td>Total Purchase Requested</td><td></td></tr><tr><td>Total Reward</td><td></td></tr><tr><td>Unmet Load</td><td></td></tr><tr><td>global_step</td><td></td></tr><tr><td>rollout/ep_len_mean</td><td></td></tr><tr><td>rollout/ep_rew_mean</td><td></td></tr><tr><td>time/fps</td><td></td></tr><tr><td>total Purchased Elec</td><td></td></tr><tr><td>train/approx_kl</td><td></td></tr><tr><td>train/clip_fraction</td><td></td></tr><tr><td>train/clip_range</td><td></td></tr><tr><td>train/entropy_loss</td><td></td></tr><tr><td>train/explained_variance</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/policy_gradient_loss</td><td></td></tr><tr><td>train/std</td><td></td></tr><tr><td>train/value_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Diesel Generator</td><td>682.30497</td></tr><tr><td>Excess Generation</td><td>542177.64378</td></tr><tr><td>Inverter total power flow</td><td>250576.77344</td></tr><tr><td>Max Demand</td><td>547.56036</td></tr><tr><td>Num Diesel Gen Actions</td><td>10</td></tr><tr><td>Num Off-Peak Purchases</td><td>1686</td></tr><tr><td>Num Peak Purchases</td><td>279</td></tr><tr><td>Num Standard Purchases</td><td>685</td></tr><tr><td>Off-Peak Purchases</td><td>300965.08527</td></tr><tr><td>Peak Purchases</td><td>38753.2685</td></tr><tr><td>Rectifier total power flow</td><td>318033.96327</td></tr><tr><td>Standard Purchases</td><td>114224.0358</td></tr><tr><td>Total Elec Purchase</td><td>453942.38957</td></tr><tr><td>Total Money spent</td><td>1583334.08427</td></tr><tr><td>Total Purchase Requested</td><td>435050.2393</td></tr><tr><td>Total Reward</td><td>-1386959.06556</td></tr><tr><td>Unmet Load</td><td>275.36282</td></tr><tr><td>global_step</td><td>1505280</td></tr><tr><td>rollout/ep_len_mean</td><td>5974.0</td></tr><tr><td>rollout/ep_rew_mean</td><td>-1392945.125</td></tr><tr><td>time/fps</td><td>599.0</td></tr><tr><td>total Purchased Elec</td><td>453942.38957</td></tr><tr><td>train/approx_kl</td><td>0.01916</td></tr><tr><td>train/clip_fraction</td><td>0.17227</td></tr><tr><td>train/clip_range</td><td>0.2</td></tr><tr><td>train/entropy_loss</td><td>0.76453</td></tr><tr><td>train/explained_variance</td><td>0.91417</td></tr><tr><td>train/learning_rate</td><td>0.01</td></tr><tr><td>train/loss</td><td>325760.65625</td></tr><tr><td>train/policy_gradient_loss</td><td>0.00379</td></tr><tr><td>train/std</td><td>0.11301</td></tr><tr><td>train/value_loss</td><td>1093763.625</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">polished-dew-247</strong> at: <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/5qjbfp0i' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/5qjbfp0i</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 3 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231018_221726-5qjbfp0i/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:5qjbfp0i). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/EEE4022S_BNKJUL001_Thesis/wandb/run-20231018_230323-9kl043kr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/9kl043kr' target=\"_blank\">royal-grass-252</a></strong> to <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/9kl043kr' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/9kl043kr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AC load</td><td></td></tr><tr><td>DC load</td><td></td></tr><tr><td>Diesel Generator</td><td></td></tr><tr><td>Excess Generation</td><td></td></tr><tr><td>Inverter total power flow</td><td></td></tr><tr><td>LoadShedding</td><td></td></tr><tr><td>Money Spent</td><td></td></tr><tr><td>Num Diesel Gen Actions</td><td></td></tr><tr><td>Num Off-Peak Purchases</td><td></td></tr><tr><td>Num Peak Purchases</td><td></td></tr><tr><td>Num Standard Purchases</td><td></td></tr><tr><td>Off-Peak Purchases</td><td></td></tr><tr><td>PV generation</td><td></td></tr><tr><td>Peak Purchases</td><td></td></tr><tr><td>Purchase Requested</td><td></td></tr><tr><td>Rectifier total power flow</td><td></td></tr><tr><td>Standard Purchases</td><td></td></tr><tr><td>Step Purchased</td><td></td></tr><tr><td>Total Reward</td><td></td></tr><tr><td>Unmet Load</td><td></td></tr><tr><td>Wind generation</td><td></td></tr><tr><td>battery_level</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AC load</td><td>113.37033</td></tr><tr><td>DC load</td><td>54.4</td></tr><tr><td>Diesel Generator</td><td>0.0</td></tr><tr><td>Excess Generation</td><td>0.0</td></tr><tr><td>Inverter total power flow</td><td>0.0</td></tr><tr><td>LoadShedding</td><td>0</td></tr><tr><td>Money Spent</td><td>0.0</td></tr><tr><td>Num Diesel Gen Actions</td><td>23</td></tr><tr><td>Num Off-Peak Purchases</td><td>600</td></tr><tr><td>Num Peak Purchases</td><td>101</td></tr><tr><td>Num Standard Purchases</td><td>162</td></tr><tr><td>Off-Peak Purchases</td><td>0.0</td></tr><tr><td>PV generation</td><td>0.0</td></tr><tr><td>Peak Purchases</td><td>0.0</td></tr><tr><td>Purchase Requested</td><td>0.0</td></tr><tr><td>Rectifier total power flow</td><td>0.0</td></tr><tr><td>Standard Purchases</td><td>0.0</td></tr><tr><td>Step Purchased</td><td>0.0</td></tr><tr><td>Total Reward</td><td>0.0</td></tr><tr><td>Unmet Load</td><td>0.0</td></tr><tr><td>Wind generation</td><td>84.94012</td></tr><tr><td>battery_level</td><td>100.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">royal-grass-252</strong> at: <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/9kl043kr' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/9kl043kr</a><br/>Synced 4 W&B file(s), 35 media file(s), 31 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231018_230323-9kl043kr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/EEE4022S_BNKJUL001_Thesis/wandb/run-20231018_230500-52wgvr4e</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/52wgvr4e' target=\"_blank\">fanciful-spaceship-253</a></strong> to <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/52wgvr4e' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/52wgvr4e</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AC load</td><td></td></tr><tr><td>DC load</td><td></td></tr><tr><td>Diesel Generator</td><td></td></tr><tr><td>Excess Generation</td><td></td></tr><tr><td>Inverter total power flow</td><td></td></tr><tr><td>LoadShedding</td><td></td></tr><tr><td>Money Spent</td><td></td></tr><tr><td>Num Diesel Gen Actions</td><td></td></tr><tr><td>Num Off-Peak Purchases</td><td></td></tr><tr><td>Num Peak Purchases</td><td></td></tr><tr><td>Num Standard Purchases</td><td></td></tr><tr><td>Off-Peak Purchases</td><td></td></tr><tr><td>PV generation</td><td></td></tr><tr><td>Peak Purchases</td><td></td></tr><tr><td>Purchase Requested</td><td></td></tr><tr><td>Rectifier total power flow</td><td></td></tr><tr><td>Standard Purchases</td><td></td></tr><tr><td>Step Purchased</td><td></td></tr><tr><td>Total Reward</td><td></td></tr><tr><td>Unmet Load</td><td></td></tr><tr><td>Wind generation</td><td></td></tr><tr><td>battery_level</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AC load</td><td>113.37033</td></tr><tr><td>DC load</td><td>54.4</td></tr><tr><td>Diesel Generator</td><td>0.0</td></tr><tr><td>Excess Generation</td><td>0.0</td></tr><tr><td>Inverter total power flow</td><td>0.0</td></tr><tr><td>LoadShedding</td><td>0</td></tr><tr><td>Money Spent</td><td>0.0</td></tr><tr><td>Num Diesel Gen Actions</td><td>0</td></tr><tr><td>Num Off-Peak Purchases</td><td>724</td></tr><tr><td>Num Peak Purchases</td><td>48</td></tr><tr><td>Num Standard Purchases</td><td>188</td></tr><tr><td>Off-Peak Purchases</td><td>0.0</td></tr><tr><td>PV generation</td><td>0.0</td></tr><tr><td>Peak Purchases</td><td>0.0</td></tr><tr><td>Purchase Requested</td><td>0.0</td></tr><tr><td>Rectifier total power flow</td><td>0.0</td></tr><tr><td>Standard Purchases</td><td>0.0</td></tr><tr><td>Step Purchased</td><td>0.0</td></tr><tr><td>Total Reward</td><td>0.0</td></tr><tr><td>Unmet Load</td><td>0.0</td></tr><tr><td>Wind generation</td><td>84.94012</td></tr><tr><td>battery_level</td><td>177.21994</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fanciful-spaceship-253</strong> at: <a href='https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/52wgvr4e' target=\"_blank\">https://wandb.ai/4022_intelligent_ems/4022_intelligent_ems/runs/52wgvr4e</a><br/>Synced 4 W&B file(s), 35 media file(s), 36 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231018_230500-52wgvr4e/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: The term does not refer to the cost in rands but rather to the reward as defined by the reward function!\n",
            "Done the Standby Test! Total cost accumulated is: -482656.3125\n",
            "Done applying the trained model! Total cost accumulated is: -474487.731165 +- 0.0\n",
            "The amount that was saved by applying the EMS agent: 8168.581334999995\n",
            "This was saved over a period of 115.0 days\n",
            "The savings represents 1.6924219415445843 % of the cost if no EMS is installed\n",
            "And it represents 1.7215579663026996 % of the cost if the EMS is installed\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    \"policy_type\": \"MlpPolicy\",\n",
        "    \"total_timesteps\": 2760,\n",
        "    \"notes\":f\"{model_note}_Eval\"\n",
        "}\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"4022_intelligent_ems\",\n",
        "    config=config,\n",
        "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    save_code=True,  # optional\n",
        ")\n",
        "\n",
        "\n",
        "eval_args = {\n",
        "                \"episode_len\"   :2760,\n",
        "                \"actual_load\"   :actual_load[6001:],\n",
        "                \"actual_gen\"    :actual_gen[6001:],\n",
        "                \"bat_threshold\" :100,\n",
        "                \"bat_cap\"       :500,\n",
        "                \"purchase_price\":purchase_price[6001:],\n",
        "                \"num_preds\"     :n_preds,\n",
        "                \"load_shedding\" :load_shedding[6001:],\n",
        "                \"wandb_log\"     : True,\n",
        "                \"train_log\"     : False\n",
        "}\n",
        "\n",
        "#define 5 environments   for training and eval\n",
        "\n",
        "eval_env = make_vec_env(EMS, n_envs = n_envs,env_kwargs = eval_args )\n",
        "\n",
        "\n",
        "#first run it with only standby (default)\n",
        "obs   = eval_env.reset()\n",
        "#ensure that the exit condition is reset\n",
        "done = [False]*n_envs\n",
        "#define the action to take\n",
        "action_standby = [-1]*n_envs\n",
        "#reset score\n",
        "standby_score = [0]*n_envs\n",
        "standby_score = np.array(standby_score).astype(np.float32)\n",
        "while not all(done):\n",
        "    #step the model with the action\n",
        "    obs,reward,done,info = eval_env.step(action_standby)\n",
        "    #accumulate the score\n",
        "    standby_score += reward\n",
        "\n",
        "\n",
        "avg_standby_score = standby_score.mean()\n",
        "\n",
        "run.finish()\n",
        "\n",
        "eval_args = {\n",
        "                \"episode_len\"   :2760,\n",
        "                \"actual_load\"   :actual_load[6001:],\n",
        "                \"actual_gen\"    :actual_gen[6001:],\n",
        "                \"bat_threshold\" :100,\n",
        "                \"bat_cap\"       :500,\n",
        "                \"purchase_price\":purchase_price[6001:],\n",
        "                \"num_preds\"     :n_preds,\n",
        "                \"load_shedding\" :load_shedding[6001:],\n",
        "                \"wandb_log\"     : True,\n",
        "                \"train_log\"     : False\n",
        "}\n",
        "\n",
        "#define 5 environments   for training and eval\n",
        "\n",
        "eval_env = make_vec_env(EMS, n_envs = n_envs,env_kwargs = eval_args )\n",
        "\n",
        "#Load model, fetch the latest (or whichever one you want from the model_dir)\n",
        "#Best A2C model:/content/drive/MyDrive/Colab Notebooks/EMSv1_1/models/A2C/EMSv1_1_A2C1010-081818.zip\n",
        "best_model =\"/content/drive/MyDrive/Colab Notebooks/EMSv2_1/models/PPO/EMSv2_1_PPO1017-110702.zip\"\n",
        "#best_model =\n",
        "#best_PPO_model\n",
        "#model_load = f\"{best_model}\"\n",
        "\n",
        "#model  = PPO.load(model_load, env = eval_env)\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"4022_intelligent_ems\",\n",
        "    config=config,\n",
        "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    save_code=True,  # optional\n",
        ")\n",
        "obs   = eval_env.reset()\n",
        "EMS_reward,EMS_std_reward = evaluate_policy(model,eval_env,n_eval_episodes = 1,deterministic=True)# callback = wandb_callback\n",
        "run.finish()\n",
        "\n",
        "print(f\"Note: The term does not refer to the cost in rands but rather to the reward as defined by the reward function!\")\n",
        "print(f\"Done the Standby Test! Total cost accumulated is: {avg_standby_score}\")\n",
        "print(f\"Done applying the trained model! Total cost accumulated is: {EMS_reward} +- {EMS_std_reward}\")\n",
        "\n",
        "savings = EMS_reward - avg_standby_score\n",
        "print(f\"The amount that was saved by applying the EMS agent: {savings}\")\n",
        "print(f\"This was saved over a period of {2760/24} days\")\n",
        "print(f\"The savings represents {(savings/(-avg_standby_score))*100} % of the cost if no EMS is installed\")\n",
        "print(f\"And it represents {(savings/(-EMS_reward))*100} % of the cost if the EMS is installed\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2f2a6a4281f43649addd9d2aa96eb08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_593de50190b8464893b1fdd0d2e67fc3",
              "IPY_MODEL_310336a9b33148b7a018647db155558c"
            ],
            "layout": "IPY_MODEL_0c75d94d374846f6a212f2b8e586cd9f"
          }
        },
        "593de50190b8464893b1fdd0d2e67fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed0fbdc7e69e427997bf962a92318d07",
            "placeholder": "",
            "style": "IPY_MODEL_9417e447e4ac4073b3707be2e7392fbc",
            "value": "1.354 MB of 1.354 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "310336a9b33148b7a018647db155558c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b0a881b4a75405cb15f7bebcdd2e66c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ce16f05fcbe4b08826965a2815104f2",
            "value": 1
          }
        },
        "0c75d94d374846f6a212f2b8e586cd9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed0fbdc7e69e427997bf962a92318d07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9417e447e4ac4073b3707be2e7392fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b0a881b4a75405cb15f7bebcdd2e66c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce16f05fcbe4b08826965a2815104f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}