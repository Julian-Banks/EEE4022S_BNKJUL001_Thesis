{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julian-Banks/EEE4022S_BNKJUL001_Thesis/blob/main/PythonWorkspace/EMSv0_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Version Notes**\n",
        "\n",
        "# **v0.2**\n",
        "**Added:**\n",
        "*  simplified load_forecast and gen_forecast to be power_bal_forecasts.\n",
        "*  combined current_load and current_gen to also show current_power_balance\n",
        "*  added proper evaluate call\n",
        "*  Monitor wrapper\n",
        "*  DummyVec wrapper\n",
        "\n",
        "**Parameters:**\n",
        "*\n",
        "*  \n",
        "\n",
        "**Results:**\n",
        "* 5% savings on PPO deterministic = true\n",
        "* 4.3% savings on PPO determnistic = false\n",
        "\n",
        "**To do:**\n",
        "* Try to use hyperparameter Optimisation\n",
        "* Try normalise\n",
        "* Try differnet models\n",
        "* Try see if discount rate can be tweaked\n",
        "* Find out how the bounds for the obs_space box effect things\n",
        "\n",
        "# **v0.1**\n",
        "**Added:**\n",
        "* added real loads, gen, tou_id's\n",
        "\n",
        "**Parameters:**\n",
        "* training episode = 6000 timesteps\n",
        "* testing episode  = 2760 timesteps\n",
        "* bat_threshold = 100\n",
        "* bat_cap = 500\n",
        "* battery_level at reset = bat_cap/2\n",
        "* num_preds = 24\n",
        "* Trained PPO for 1.65mil timesteps\n",
        "* Trained A2C for 1.2 mil timesteps\n",
        "\n",
        "**Results:**\n",
        "* PP0 - 4% improvement from standby mode\n",
        "* A2C  - -0.3% improvement. And the models after this got worse as training progressed!\n",
        "**To do:**\n",
        "* Try lower num_preds\n",
        "* Try to use hyperparameter Optimisation\n",
        "* Try normalise\n",
        "* Try differnet models\n",
        "* Try see if discount rate can be tweaked\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5DZFnZnzStt1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MXzZxVUQFroQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gymnasium\n",
        "!pip install stable_baselines3[extra]\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hu74obFw8Gts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade94c0a-ec1a-42b0-860c-788a30765691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EEE4022S_BNKJUL001_Thesis'...\n",
            "remote: Enumerating objects: 500, done.\u001b[K\n",
            "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
            "remote: Compressing objects: 100% (167/167), done.\u001b[K\n",
            "remote: Total 500 (delta 80), reused 159 (delta 40), pack-reused 290\u001b[K\n",
            "Receiving objects: 100% (500/500), 145.34 MiB | 29.15 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n",
            "Updating files: 100% (320/320), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/Julian-Banks/EEE4022S_BNKJUL001_Thesis\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to update the rep\n",
        "%cd /content/EEE4022S_BNKJUL001_Thesis\n",
        "! git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IBm2Ge7rrfe",
        "outputId": "baa59a88-4be2-42f0-8199-fe758c57da89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EEE4022S_BNKJUL001_Thesis\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to save the code afterwards\n",
        "'''\n",
        "%cd /content/EEE4022S_BNKJUL001_Thesis\n",
        "\n",
        "Message = \"Changes to EMSv0_2\"\n",
        "\n",
        "! git add.\n",
        "! git commit -m \"{Message}\"\n",
        "! git push\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OYtyqOTcscTG",
        "outputId": "8538bb86-63ac-40f4-b53d-775388bc0dd9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n%cd /content/EEE4022S_BNKJUL001_Thesis\\n\\nMessage = \"Changes to EMSv0_2\"\\n\\n! git add.\\n! git commit -m \"{Message}\"\\n! git push\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI52iVVCCPaf",
        "outputId": "5ee80e81-8f4e-40d8-d4fc-f5ec4b2744dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "#import needed libarys\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gymnasium import spaces\n",
        "import datetime\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "from google.colab import drive\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define our environment class!!**"
      ],
      "metadata": {
        "id": "Q99_jLMvwuZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s2iW-k26FIbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec45746-ee35-439e-eaec-ca5e7eb85514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class EMSv0_2(gym.Env):\n",
        "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self,bat_threshold = 0.1, bat_cap = 1, actual_load = \"none\", actual_gen = \"none\", purchase_price = [1,1,1,1,1,1,1,1,2,2,2,2] , episode_len = 8760,num_preds = 24,render_mode = \"none\"):\n",
        "\n",
        "        super(EMSv0_2, self).__init__()\n",
        "\n",
        "        #define time frame\n",
        "        self.current_step = 0\n",
        "        self.final_step = int(episode_len)-num_preds-2 #one years worth of steps\n",
        "\n",
        "        #Might make a function for these\n",
        "        #fill all of the actual loads NB!!! is just random for now NB!!! is normalised 0-1\n",
        "        if isinstance(actual_load,str) :\n",
        "            self.actual_load = np.random.rand(self.final_step+num_preds+1).astype(np.float32) #will load from a file or something\n",
        "        else:\n",
        "            self.actual_load = actual_load[:episode_len]\n",
        "\n",
        "        #fill all of the actual generation steps.\n",
        "        if isinstance(actual_gen,str):\n",
        "            self.actual_gen  = np.random.rand(self.final_step+num_preds+1).astype(np.float32) #will load from file or something\n",
        "        else:\n",
        "            self.actual_gen  = actual_gen[:episode_len]\n",
        "\n",
        "        #define the purchase price for every step of the year\n",
        "        purchase_price = np.array(purchase_price).astype(np.float32)\n",
        "        repetitions    = (self.final_step+num_preds+1) // len(purchase_price)\n",
        "        remainder      = (self.final_step+num_preds+1) % len(purchase_price)\n",
        "        self.purchase_price =np.concatenate([purchase_price]*repetitions+[purchase_price[:remainder]])#need to read in from somewhere\n",
        "\n",
        "        #define var for storing the excess gen\n",
        "        self.excess_gen = 0\n",
        "        #define a var for determine amount purchased per step (dont want to make it total as this will incure growing penalties for the Agent if used in reward structure)\n",
        "        self.step_purchased = 0\n",
        "        #define the battery max capacity\n",
        "        self.bat_cap = bat_cap\n",
        "        #define the battery low threshold\n",
        "        self.bat_threshold = np.float32(bat_threshold)\n",
        "        #define default action\n",
        "        self.default_action = 0\n",
        "        #define actions and observations space\n",
        "        n_actions = 2 # keeping it simple\n",
        "\n",
        "        self.num_preds = num_preds # day ahead predictions\n",
        "        self.action_space = spaces.Discrete(n_actions)\n",
        "        # Dict space to store all the different things\n",
        "        self.observation_space = spaces.Dict({\n",
        "                \"power_bal_forecast\": gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1,num_preds), dtype=np.float32),\n",
        "                \"price_forecast\": gym.spaces.Box(low=0, high=np.inf, shape=(1,num_preds+1), dtype=np.float32),\n",
        "                \"bat_level\": gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),\n",
        "                \"current_power_bal\": gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32),\n",
        "                })\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        #update the current state with the action (needs to be done before current_step is inc since we want to apply the action to the previous step to get the current state)\n",
        "        self.update_state(action)\n",
        "        #Calculate reward from the action\n",
        "        reward = self.calc_reward()\n",
        "\n",
        "        #inc time step into Future\n",
        "        self.current_step += 1\n",
        "        #get next observation (for next time step)\n",
        "        observation = self.get_obs()\n",
        "        #Set terminated to False since there are no failure states\n",
        "        self.terminated = False\n",
        "        #Check if timelimit reached\n",
        "        self.truncated = False if self.current_step<self.final_step else True\n",
        "        #dont know what to put into info for now\n",
        "        info = {}\n",
        "        return observation, reward, self.terminated, self.truncated, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed = seed, options=options)\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.terminated = False\n",
        "        self.truncated = False\n",
        "\n",
        "        #reset these cause I dont want the model to just memorise the random data (so it gets changed every reset)\n",
        "        #fill all of the actual loads NB!!! is just random for now NB!!! is normalised 0-1\n",
        "        #self.actual_load = np.random.rand(self.final_step+self.num_preds+1).astype(np.float32) #will load from a file or something\n",
        "        #fill all of the actual generation steps.\n",
        "        #self.actual_gen   = np.random.rand(self.final_step+self.num_preds+1).astype(np.float32) #will load from file or something\n",
        "\n",
        "\n",
        "        #reset the state\n",
        "        self.battery_level = self.bat_cap/2\n",
        "        self.current_load = self.actual_load[0]\n",
        "        self.excess_gen = 0\n",
        "        self.step_purchased = 0\n",
        "        #get the first observation\n",
        "        observation = self.get_obs()\n",
        "        #Still don't know what to do with info\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def render(self):\n",
        "        #Reaaaaalllyyyy want to render something, Maybe the curent load as a point, the forecasts as a plot and the bat levels as bar\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        #don't think i need this for my application\n",
        "        pass\n",
        "\n",
        "    def update_state(self, action):\n",
        "        #Update current state with actions\n",
        "        if action == 0: #do nothing action\n",
        "            self.standby()\n",
        "        elif action == 1: #buy from Grid\n",
        "            self.purchase()\n",
        "        else:  #error case\n",
        "            raise ValueError(\n",
        "              f\"Received invalid action = {action} which is not part of the action space.\"\n",
        "            )\n",
        "        #case list for each action?\n",
        "\n",
        "    def calc_reward(self):\n",
        "        #Calculate reward based on the state\n",
        "        reward = -self.step_purchased*self.purchase_price[self.current_step]\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_obs(self):\n",
        "        #Fill the observation space with the next observation\n",
        "\n",
        "        #Get Forecasts Will probaly write a function for this? idk maybe a schlep to return all the info\n",
        "        load_forecast  = np.array( [self.actual_load[self.current_step+1: self.current_step + self.num_preds+1]] , dtype = np.float32) #will load from a file or something\n",
        "        if load_forecast.shape != (1,self.num_preds):\n",
        "            print(f\"load_forecast shape is {load_forecast.shape} but it should be {(1, self.num_preds)}. Current step is {self.current_step}\")\n",
        "        gen_forecast   = np.array( [self.actual_gen[self.current_step+1: self.current_step + self.num_preds+1]] , dtype = np.float32) #will load from a file or something\n",
        "        if gen_forecast.shape != (1,self.num_preds):\n",
        "            print(f\"gen_forecast shape is {gen_forecast.shape} but it should be {(1, self.num_preds)}. Current step is {self.current_step}\")\n",
        "        #calculate the power forecast\n",
        "        power_bal_forecast = gen_forecast-load_forecast\n",
        "        #get the prices for the current frame and the next 24 hours. Maybe will cut this down since that seems like a lot of info\n",
        "        price_forecast = np.array( [self.purchase_price[self.current_step:self.current_step+self.num_preds+1]] , dtype = np.float32)\n",
        "        #Just for readibility of the dict object\n",
        "        bat_level      = np.array([self.battery_level] , dtype= np.float32)\n",
        "\n",
        "        #calculate the current power balance\n",
        "        current_load   = np.array([self.actual_load[self.current_step]], dtype = np.float32)\n",
        "        current_gen    = np.array([self.actual_gen[self.current_step]], dtype  = np.float32)\n",
        "        current_power_bal = current_gen - current_load\n",
        "\n",
        "\n",
        "\n",
        "        obs = dict({\n",
        "                \"bat_level\":      bat_level,\n",
        "                \"current_power_bal\" :   current_power_bal,\n",
        "                \"power_bal_forecast\":  power_bal_forecast,\n",
        "                \"price_forecast\": price_forecast,\n",
        "        })\n",
        "        return obs\n",
        "\n",
        "    def standby(self):\n",
        "        #ems stands by, load is met by generation, battery and then grid\n",
        "        #if there is excess generation it is used to charge the batteries\n",
        "\n",
        "        #define step_gen and step_load for readability\n",
        "        step_gen  =  self.actual_gen[self.current_step]\n",
        "        step_load =  self.actual_load[self.current_step]\n",
        "        battery   =  self.battery_level\n",
        "        #check for gen meeting load\n",
        "        if step_load <= step_gen :\n",
        "            #set the purchased elect to 0 since gen meets load\n",
        "            self.step_purchased = 0\n",
        "            #calulate the excess elec that was generated\n",
        "            step_excess = step_gen - step_load\n",
        "            #check if battery needs to be charged\n",
        "            if battery < self.bat_cap :\n",
        "                #check if the excess amount that was generated is less than the available capacity\n",
        "                if self.bat_cap-battery-step_excess > 0:\n",
        "                    self.battery_level += step_excess\n",
        "                else:\n",
        "                    #if the excess is greater than the availability then charge till full\n",
        "                    self.battery_level = self.bat_cap\n",
        "                    #set step excess to excess minus the amount used to charge\n",
        "                    step_excess -= (self.bat_cap-battery)\n",
        "                    self.excess_gen += step_excess\n",
        "            else:\n",
        "                #if the battery is full then just inc excess_gen\n",
        "                self.excess_gen += step_excess\n",
        "        else:\n",
        "            #if the generation does not meet load\n",
        "            step_shortfall = step_load - step_gen\n",
        "            #checking if battery is above a threshold.\n",
        "            if battery > self.bat_threshold:\n",
        "                #check if battery has enough capacity to meet the load\n",
        "                if battery - step_shortfall >= self.bat_threshold:\n",
        "                    #if it does then subtract the shortfall from battery level\n",
        "                    self.battery_level -= step_shortfall\n",
        "                    #set the purchased variable to 0 since nothing was purchased\n",
        "                    self.step_purchased = 0\n",
        "                else:\n",
        "                    #set the battery to min value and purchase the rest from the grid\n",
        "                    self.battery_level = self.bat_threshold\n",
        "                    #calculate how much needs to be purchased\n",
        "                    step_shortfall -= (battery - self.bat_threshold)\n",
        "                    self.step_purchased = step_shortfall\n",
        "            else:\n",
        "                #no battery available, therefore everything needs to be bought from the grid.\n",
        "                self.step_purchased = step_shortfall\n",
        "\n",
        "    def purchase(self):\n",
        "        #purchase electricity to charge battery even if there is enough generation (I assume this will be used to buy at lower prices)\n",
        "        #get values for readability\n",
        "        step_load = self.actual_load[self.current_step]\n",
        "        step_gen  = self.actual_gen[self.current_step]\n",
        "        battery = self.battery_level\n",
        "\n",
        "        #calculate the total power need (the load plus the amount that the battery needs to charge)\n",
        "        total_need = step_load + (self.bat_cap-battery)\n",
        "        #if the generation is less than the need then purchase the remainder\n",
        "        if step_gen<total_need:\n",
        "            #purchashing the shortfall\n",
        "            self.step_purchased = total_need - step_gen\n",
        "            #setting the battery levels to full\n",
        "            self.battery_level = self.bat_cap\n",
        "        else:\n",
        "            #if the gen is enough then set purchase to 0\n",
        "            self.step_purchased  = 0\n",
        "            #set the battery to fully charged\n",
        "            self.battery_level = self.bat_cap\n",
        "            #inc excess_gen by caluclating the excess between the step gen and the total need (includes amount needed to charge the battery)\n",
        "            self.excess_gen += (step_gen - total_need)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RUZKr7RrN-u"
      },
      "source": [
        "Check the environment with stable_baselines3 check_env."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v4GSJ8LsidPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928a5323-22d3-429b-e61f-0437205178f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation power_bal_forecast has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation price_forecast has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "env = EMSv0_2()\n",
        "check_env(env,warn = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG4gB2cM7tDY"
      },
      "source": [
        "**Load in the data for our specific microgrid.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0X9RBdXC_2PD"
      },
      "outputs": [],
      "source": [
        "#need to import data from Github\n",
        "path_data = \"/content/EEE4022S_BNKJUL001_Thesis/PythonWorkspace/dataClean.csv\"\n",
        "data = pd.read_csv(path_data)\n",
        "\n",
        "path_gen = \"/content/EEE4022S_BNKJUL001_Thesis/Generation/BNKJUL001_Thesis_solarGen500kWHomer.csv\"\n",
        "data_gen = pd.read_csv(path_gen)\n",
        "\n",
        "#Not actually using this rn but will be soon :)\n",
        "path_shedding = \"/content/EEE4022S_BNKJUL001_Thesis/MatlabWorkSpace/loadShedding2022.csv\"\n",
        "data_shedding = pd.read_csv(path_shedding)\n",
        "load_shedding = data_shedding['LoadShedding'].values.astype(np.float32)\n",
        "\n",
        "actual_gen = data_gen['PV_Out'].values.astype(np.float32)\n",
        "actual_load = data['AC'].values.astype(np.float32)\n",
        "purchase_price = data['tou_id'].values.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kvh0dymrVgV"
      },
      "source": [
        "Evaluate the base model (no EMS, just using standby mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vMEdXW4otUB",
        "outputId": "a8f9ae87-fbfe-4b97-9065-ef0dfabb8add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reset observation space looks like: {'bat_level': array([250.], dtype=float32), 'current_power_bal': array([-96.855], dtype=float32), 'power_bal_forecast': array([[-97.053, -97.229, -96.859]], dtype=float32), 'price_forecast': array([[1., 1., 1., 1.]], dtype=float32)}\n",
            "After action 0 the observation space looks like {'bat_level': array([153.14499], dtype=float32), 'current_power_bal': array([-97.053], dtype=float32), 'power_bal_forecast': array([[-97.229, -96.859, -98.018]], dtype=float32), 'price_forecast': array([[1., 1., 1., 1.]], dtype=float32)}\n",
            "The reward we recieved was 0.0\n"
          ]
        }
      ],
      "source": [
        "#define the base environment\n",
        "base_env = EMSv0_2(episode_len = 6000, actual_load = actual_load, actual_gen = actual_gen, bat_threshold = 100, bat_cap = 500, purchase_price = purchase_price,num_preds = 3)\n",
        "#going to print out a bunch of things to test the different spaces.\n",
        "obs,_    = base_env.reset()\n",
        "print(f\"The reset observation space looks like: {obs}\")\n",
        "action_standby = 0\n",
        "obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "print(f\"After action {action_standby} the observation space looks like {obs}\")\n",
        "print(f\"The reward we recieved was {reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A loop to get an average reward for the base model only perfoming the standby option"
      ],
      "metadata": {
        "id": "-0iOsrI-0qGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward,std_reward = evaluate_policy(model,base_env,n_eval_episodes = 100)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +- {std_reward}\")"
      ],
      "metadata": {
        "id": "6ZzBAQFX2_DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reset the environment and save the obs\n",
        "#going to run it 100 times to get a benchmark\n",
        "#reset score\n",
        "score = 0\n",
        "for step in range(1000):\n",
        "    obs,_    = base_env.reset()\n",
        "    #ensure that the exit condition is reset\n",
        "    truncated = False\n",
        "    #define the action to take\n",
        "    action_standby = 0\n",
        "\n",
        "    while not truncated:\n",
        "        obs,reward,terminated,truncated,info = base_env.step(action_standby)\n",
        "        score += reward\n",
        "\n",
        "print(f\"Done iteration! Total reward accumulated is: {score/step}\")"
      ],
      "metadata": {
        "id": "BztNxs9w0nRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41cf7dd-2869-4bfe-c1b1-239333d8f8db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done iteration! Total reward accumulated is: -880769.0387836567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRy0KQKOriqk"
      },
      "source": [
        "Connect to drive, create new directories for the logs and models to be saved in.\n",
        "\n",
        "Open tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqy-glMavKA6"
      },
      "outputs": [],
      "source": [
        "\n",
        "#mount the drive\n",
        "drive.mount('/content/drive')\n",
        "#define paths to logs and model saves\n",
        "model_type = \"PPO\"\n",
        "version    = \"EMSv0_2\"\n",
        "model_dir = f\"/content/drive/MyDrive/Colab Notebooks/{version}/models/{model_type}/\"\n",
        "log_dir   = f\"/content/drive/MyDrive/Colab Notebooks/{version}/models/{model_type}logs/\"\n",
        "\n",
        "#make the appropriate directory if it does not exist\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"{log_dir}\" --load_fast=false --port 8008\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pAj6-n04zyt"
      },
      "source": [
        "**LOAD OR MAKE MODEL HERE!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gQDhUNGWeum4"
      },
      "outputs": [],
      "source": [
        "#create a new environment to train the model in.\n",
        "train_env = EMSv0_2(episode_len = 6000, actual_load = actual_load, actual_gen = actual_gen, bat_threshold = 100, bat_cap = 500, purchase_price = purchase_price,num_preds = 24)\n",
        "\n",
        "train_env = Monitor(train_env)\n",
        "train_env = DummyVecEnv(train_env)\n",
        "#Create the model with the MultiInputPolicy, use the training env, verbose is off because tensorboard loging is enabled\n",
        "#model = PPO(\"MultiInputPolicy\",train_env, verbose = 0, tensorboard_log = log_dir)\n",
        "\n",
        "#Load model, fetch the latest (or whichever one you want from the model_dir)\n",
        "#PPO latest: /content/drive/MyDrive/Colab Notebooks/EMSv0_2/models/PPO/EMSv0_2_PPO20231005-083718.zip\n",
        "model_load = f\"{model_dir}/EMSv0_2_PPO20231005-070537.zip\"\n",
        "model  = PPO.load(model_load, env = train_env)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infinite while loop to train  model.**"
      ],
      "metadata": {
        "id": "vEDG0IiVvxeH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5dU9TLFzhxD"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    #define the name for the specific log\n",
        "    log_name = f\"{version}_{model_type}\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    #make the model learn, set the reset to false so that it keeps its old learning\n",
        "    model.learn(total_timesteps= 30000, tb_log_name = log_name,reset_num_timesteps=False)\n",
        "    model.save(f\"{model_dir}{log_name}\")\n",
        "    #open tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a new test environment and load up the best performing model to test it.**"
      ],
      "metadata": {
        "id": "C9AGcLlsv4N7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "inDHszZaxBUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63a1ad3-0258-4a9c-fd0d-32edfeeba198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: The term does not refer to the cost in rands but rather to the reward as defined by the reward function!\n",
            "Done the Standby Test! Total cost accumulated is: -308596.6616592407\n",
            "Done applying the trained model! Total cost accumulated is: -295807.0799106598 +- 2118.4823226584476\n",
            "The amount that was saved by applying the EMS agent: 12789.58174858091\n",
            "This was saved over a period of 115.0 days\n",
            "The savings represents 4.144432956537764 % of the cost if no EMS is installed\n",
            "And it represents 4.323622596336653 % of the cost if the EMS is installed\n"
          ]
        }
      ],
      "source": [
        "#define a test environment\n",
        "test_env = EMSv0_2(episode_len = 2760, actual_load = actual_load[6001:], actual_gen = actual_gen[6001:], bat_threshold = 100, bat_cap = 500, purchase_price = purchase_price[6001:],num_preds = 24)\n",
        "#reset the environment and save the obs\n",
        "\n",
        "#Load model, fetch the latest (or whichever one you want from the model_dir)\n",
        "#Best A2C model:\n",
        "best_PPO_model = \"EMSv0_2_PPO20231005-083718.zip\"\n",
        "\n",
        "model_type = \"PPO\"\n",
        "version    = \"EMSv0_2\"\n",
        "model_dir = f\"/content/drive/MyDrive/Colab Notebooks/{version}/models/{model_type}/\"\n",
        "log_dir   = f\"/content/drive/MyDrive/Colab Notebooks/{version}/models/{model_type}logs/\"\n",
        "\n",
        "model_load = f\"{model_dir}/{best_PPO_model}\"\n",
        "model  = PPO.load(model_load, env = test_env)\n",
        "\n",
        "\n",
        "#first run it with only standby (default)\n",
        "obs,_    = test_env.reset()\n",
        "#ensure that the exit condition is reset\n",
        "truncated = False\n",
        "#define the action to take\n",
        "action_standby = 0\n",
        "#reset score\n",
        "standby_score = 0\n",
        "while not truncated:\n",
        "    #step the model with the action\n",
        "    obs,reward,terminated,truncated,info = test_env.step(action_standby)\n",
        "    #accumulate the score\n",
        "    standby_score += reward\n",
        "\n",
        "EMS_reward,EMS_std_reward = evaluate_policy(model,test_env,n_eval_episodes = 100,deterministic=False)\n",
        "\n",
        "\n",
        "print(f\"Note: The term does not refer to the cost in rands but rather to the reward as defined by the reward function!\")\n",
        "print(f\"Done the Standby Test! Total cost accumulated is: {standby_score}\")\n",
        "print(f\"Done applying the trained model! Total cost accumulated is: {EMS_reward} +- {EMS_std_reward}\")\n",
        "\n",
        "savings = EMS_reward - standby_score\n",
        "print(f\"The amount that was saved by applying the EMS agent: {savings}\")\n",
        "print(f\"This was saved over a period of {2760/24} days\")\n",
        "print(f\"The savings represents {(savings/(-standby_score))*100} % of the cost if no EMS is installed\")\n",
        "print(f\"And it represents {(savings/(-EMS_reward))*100} % of the cost if the EMS is installed\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLtoaDaZSAlQ46zYesQEJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}